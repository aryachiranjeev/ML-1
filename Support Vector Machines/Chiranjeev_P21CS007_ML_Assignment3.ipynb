{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chiranjeev_P21CS007_ML_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_g1foyCXa2iI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zaBXIZLZ63N"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzZRARrVaDCw"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "X,Y = load_iris( return_X_y=True)\n",
        "Y = Y[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTJ06DVWaE60"
      },
      "source": [
        "Y1 = []\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == 0:\n",
        "    Y1.append(-1)\n",
        "  elif Y[i] == 1:\n",
        "    Y1.append(1)\n",
        "    \n",
        "Y1 = np.array(Y1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFIufNAqab1i",
        "outputId": "52459adf-90ac-4532-caa2-8af32b4e56a7"
      },
      "source": [
        "X1 = np.hstack((X[0:100],np.ones(100).reshape(100,1)))\n",
        "print(X1.shape)\n",
        "print(Y1.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 5)\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt3DWwMVafvc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X1,Y1,test_size = 0.3,shuffle = True,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzSIo1OaagRN",
        "outputId": "37fed41c-6381-4738-c08b-38a2f6bfa9f9"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 5)\n",
            "(30, 5)\n",
            "(70,)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI0IqMZAa7Il"
      },
      "source": [
        "# Hard SVM Using Dual Formulation (cvxopt_solvers.qp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1mBooHOa6TH"
      },
      "source": [
        "#Linear Kernel\n",
        "def linear_kernel(x1,x2):\n",
        "    return np.dot(x1, x2.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVe2VGsxa_D9",
        "outputId": "daeadf4c-25ac-40e8-e8da-52c465000b14"
      },
      "source": [
        "X_sep = x_train\n",
        "Y_sep = y_train\n",
        "print(X_sep.shape)\n",
        "print(Y_sep.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 5)\n",
            "(70,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgU879KNbCXA"
      },
      "source": [
        "from cvxopt import matrix,solvers\n",
        "phi = linear_kernel(y_train.reshape(-1,1)*x_train,y_train.reshape(-1,1)*x_train) \n",
        "P=matrix(phi)\n",
        "q = matrix(-np.ones(len(x_train)).reshape(len(x_train),1))  \n",
        "h = matrix(np.zeros(len(x_train))) \n",
        "g = -np.identity(len(x_train)) \n",
        "G = matrix(g) \n",
        "A = matrix(y_train.reshape(1,-1).astype(float))\n",
        "b = matrix(np.zeros(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk02pwBTbDpL",
        "outputId": "b0de1263-cdf8-4109-9575-03f914a89d78"
      },
      "source": [
        "sol=solvers.qp(P=P,G=G,h=h,q=q,A=A,b=b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -2.7002e+00 -4.8935e+00  1e+02  1e+01  2e+00\n",
            " 1: -1.0501e+00 -1.9972e+00  1e+01  7e-01  1e-01\n",
            " 2: -3.3980e-01 -1.2216e+00  1e+00  8e-03  1e-03\n",
            " 3: -5.4549e-01 -7.7444e-01  2e-01  2e-03  3e-04\n",
            " 4: -6.4485e-01 -8.0322e-01  2e-01  5e-04  7e-05\n",
            " 5: -7.3077e-01 -7.5458e-01  2e-02  6e-05  9e-06\n",
            " 6: -7.4775e-01 -7.4813e-01  4e-04  7e-07  9e-08\n",
            " 7: -7.4805e-01 -7.4806e-01  4e-06  7e-09  9e-10\n",
            " 8: -7.4806e-01 -7.4806e-01  4e-08  7e-11  9e-12\n",
            "Optimal solution found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0xEkAmsdc2J",
        "outputId": "b4a00374-d66c-47bd-d3d4-186c994ddadd"
      },
      "source": [
        "print(sol['x'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.61e-09]\n",
            "[ 1.16e-09]\n",
            "[ 3.09e-10]\n",
            "[ 9.83e-10]\n",
            "[ 1.71e-09]\n",
            "[ 1.33e-09]\n",
            "[ 3.06e-10]\n",
            "[ 3.05e-10]\n",
            "[ 7.99e-10]\n",
            "[ 7.26e-10]\n",
            "[ 1.06e-09]\n",
            "[ 1.91e-09]\n",
            "[ 1.21e-09]\n",
            "[ 4.07e-10]\n",
            "[ 1.10e-09]\n",
            "[ 9.10e-10]\n",
            "[ 6.29e-10]\n",
            "[ 4.04e-09]\n",
            "[ 3.67e-10]\n",
            "[ 6.93e-10]\n",
            "[ 1.83e-07]\n",
            "[ 1.92e-09]\n",
            "[ 9.55e-10]\n",
            "[ 1.13e-09]\n",
            "[ 1.90e-09]\n",
            "[ 2.86e-10]\n",
            "[ 1.01e-09]\n",
            "[ 8.43e-10]\n",
            "[ 8.17e-10]\n",
            "[ 3.92e-10]\n",
            "[ 2.59e-10]\n",
            "[ 3.94e-10]\n",
            "[ 2.53e-10]\n",
            "[ 1.54e-09]\n",
            "[ 2.53e-10]\n",
            "[ 4.25e-10]\n",
            "[ 1.03e-09]\n",
            "[ 2.85e-10]\n",
            "[ 3.57e-10]\n",
            "[ 3.31e-10]\n",
            "[ 1.02e-09]\n",
            "[ 7.67e-02]\n",
            "[ 2.63e-10]\n",
            "[ 9.47e-10]\n",
            "[ 7.48e-01]\n",
            "[ 2.10e-09]\n",
            "[ 2.98e-10]\n",
            "[ 8.05e-10]\n",
            "[ 3.53e-10]\n",
            "[ 4.68e-10]\n",
            "[ 2.61e-10]\n",
            "[ 3.15e-10]\n",
            "[ 8.09e-10]\n",
            "[ 2.46e-09]\n",
            "[ 1.58e-09]\n",
            "[ 2.18e-10]\n",
            "[ 1.04e-09]\n",
            "[ 9.29e-10]\n",
            "[ 6.71e-01]\n",
            "[ 2.78e-10]\n",
            "[ 2.75e-10]\n",
            "[ 3.26e-10]\n",
            "[ 2.45e-10]\n",
            "[ 4.85e-10]\n",
            "[ 2.16e-09]\n",
            "[ 6.60e-10]\n",
            "[ 4.19e-10]\n",
            "[ 7.26e-10]\n",
            "[ 4.11e-10]\n",
            "[ 2.90e-10]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2QXx973djFq",
        "outputId": "b9eeb52c-bf73-4321-cae8-0bd805db18aa"
      },
      "source": [
        "alphas = np.array(sol['x']) # return alphas\n",
        "\n",
        "compared_alphas = [alphas > 1e-7] \n",
        "\n",
        "print(\"number_support_vectors:\",np.sum(compared_alphas))\n",
        "support_vector_idx = (alphas > 1e-7).flatten()\n",
        "sv = x_train[support_vector_idx]\n",
        "sv_y = y_train[support_vector_idx]\n",
        "sv_alphas = alphas[support_vector_idx]\n",
        "\n",
        "weights = []\n",
        "for i in range(len(x_train)):\n",
        "  weights.append(x_train[i]*y_train[i]*alphas[i][0])\n",
        "\n",
        "\n",
        "weights = np.sum(np.array(weights),axis=0).reshape(-1,1)\n",
        "print(weights)\n",
        "bias = sv_y.reshape(len(sv_y),1) - np.matmul(sv,weights)\n",
        "bias = bias[0] \n",
        "print(bias)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_support_vectors: 4\n",
            "[[ 4.60343900e-02]\n",
            " [-5.21722483e-01]\n",
            " [ 1.00316482e+00]\n",
            " [ 4.64179589e-01]\n",
            " [-2.63029863e-17]]\n",
            "[-1.44595771]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BccCbDeHeNoj",
        "outputId": "cd4c8bc1-5162-43ec-9d61-7d6513d240cc"
      },
      "source": [
        "pred = np.sum(linear_kernel(sv, x_train) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_train = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train, y_pred = y_pred_train))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train, y_pred = y_pred_train))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70\n",
            "0\n",
            "confusion_matrix:\n",
            " [[33  0]\n",
            " [ 0 37]]\n",
            "33 0 0 37\n",
            "training accuracy: 100.0 %\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPlebVF8eXwA",
        "outputId": "2fa7f44e-0176-41da-8938-beec13173d12"
      },
      "source": [
        "pred = np.sum(linear_kernel(sv, x_test) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_test = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test, y_pred = y_pred_test))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test, y_pred = y_pred_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "0\n",
            "confusion_matrix:\n",
            " [[17  0]\n",
            " [ 0 13]]\n",
            "17 0 0 13\n",
            "testing accuracy: 100.0 %\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbPliFjQeaAa",
        "outputId": "f87ffd4f-abd6-4019-bfb5-332e8d6bc3a4"
      },
      "source": [
        "max_margin_distance = 2 / np.linalg.norm(weights)\n",
        "\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"support_vectors:\",sv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_margin_distance 1.6351115307261888\n",
            "postive_margin_distance, negative_margin_distance: 0.8175557653630944\n",
            "support_vectors: [[4.8 3.4 1.9 0.2 1. ]\n",
            " [4.5 2.3 1.3 0.3 1. ]\n",
            " [5.1 2.5 3.  1.1 1. ]\n",
            " [5.1 3.3 1.7 0.5 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdf_mLjkfAki"
      },
      "source": [
        "# Soft SVM Using Dual Formulation (cvxopt_solvers.qp)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLcZGs5efC2N",
        "outputId": "70042d59-5679-4045-9fa5-f4628a5b6952"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "\n",
        "BreastCancer = pd.read_csv(\"/content/BreastCancer.csv\")\n",
        "BreastCancer = BreastCancer.drop(['Unnamed: 32'], axis=1)\n",
        "X1 = BreastCancer.iloc[:,2:].values\n",
        "Y = BreastCancer.iloc[:,1].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "#### scaler = MinMaxScaler()\n",
        "scaler.fit(X1)\n",
        "X1 = scaler.transform(X1)\n",
        "\n",
        "\n",
        "\n",
        "X1 = np.hstack((X1,np.ones(len(X1)).reshape(len(X1),1)))\n",
        "Y1 = []\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == 'M':\n",
        "    Y1.append(-1)\n",
        "  elif Y[i] == 'B':\n",
        "    Y1.append(1)\n",
        "    \n",
        "Y1 = np.array(Y1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X1,Y1,test_size = 0.2,shuffle = True,random_state = 42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(455, 31)\n",
            "(455,)\n",
            "(114, 31)\n",
            "(114,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "EjgIU9ctfNZ2",
        "outputId": "77872809-cfe0-4a04-d6df-611633e2bf5d"
      },
      "source": [
        "BreastCancer.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0    842302         M  ...          0.4601                  0.11890\n",
              "1    842517         M  ...          0.2750                  0.08902\n",
              "2  84300903         M  ...          0.3613                  0.08758\n",
              "3  84348301         M  ...          0.6638                  0.17300\n",
              "4  84358402         M  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1_beh4EfYNv",
        "outputId": "90dcc54f-a703-40f8-b42a-e4bb10531cab"
      },
      "source": [
        "BreastCancer.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                         0\n",
              "diagnosis                  0\n",
              "radius_mean                0\n",
              "texture_mean               0\n",
              "perimeter_mean             0\n",
              "area_mean                  0\n",
              "smoothness_mean            0\n",
              "compactness_mean           0\n",
              "concavity_mean             0\n",
              "concave points_mean        0\n",
              "symmetry_mean              0\n",
              "fractal_dimension_mean     0\n",
              "radius_se                  0\n",
              "texture_se                 0\n",
              "perimeter_se               0\n",
              "area_se                    0\n",
              "smoothness_se              0\n",
              "compactness_se             0\n",
              "concavity_se               0\n",
              "concave points_se          0\n",
              "symmetry_se                0\n",
              "fractal_dimension_se       0\n",
              "radius_worst               0\n",
              "texture_worst              0\n",
              "perimeter_worst            0\n",
              "area_worst                 0\n",
              "smoothness_worst           0\n",
              "compactness_worst          0\n",
              "concavity_worst            0\n",
              "concave points_worst       0\n",
              "symmetry_worst             0\n",
              "fractal_dimension_worst    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEHEvb5hfZWa"
      },
      "source": [
        "#Linear Kernel\n",
        "def linear_kernel(x1,x2):\n",
        "    return np.dot(x1, x2.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ul6mFJfjK3"
      },
      "source": [
        "**Regularization parameter = 0.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tlA4dcfjgA"
      },
      "source": [
        "from cvxopt import matrix,solvers\n",
        "C_regularization_parameter = 0.1\n",
        "phi = linear_kernel(y_train.reshape(-1,1)*x_train,y_train.reshape(-1,1)*x_train)  # kernel\n",
        "P = matrix(phi)\n",
        "q = matrix(-np.ones(len(x_train)).reshape(len(x_train),1)) # coefficients of alpha_i \n",
        "h = matrix(np.hstack((np.zeros(len(x_train)),np.ones(len(x_train))*C_regularization_parameter))) # RHS of 0 <= alpha <= C\n",
        "g = np.vstack((-np.identity(len(x_train)),np.identity(len(x_train)))) # LHS of  0 <= alpha <= C\n",
        "G = matrix(g)\n",
        "A = matrix(y_train.reshape(1,-1).astype(float))\n",
        "b = matrix(np.zeros(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZnHd4xQgVMF",
        "outputId": "b249008b-694e-49b5-d48a-0abe5216df73"
      },
      "source": [
        "sol=solvers.qp(P=P,q=q,G=G,h=h,A=A,b=b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3199e+01 -8.5783e+01  3e+03  2e+01  6e-14\n",
            " 1: -1.0840e+01 -7.8855e+01  2e+02  2e+00  4e-14\n",
            " 2: -7.3600e+00 -5.2311e+01  1e+02  6e-01  2e-14\n",
            " 3: -4.8008e+00 -2.4938e+01  3e+01  1e-01  6e-15\n",
            " 4: -3.7206e+00 -1.0954e+01  1e+01  5e-02  3e-15\n",
            " 5: -3.3812e+00 -5.3502e+00  3e+00  1e-02  3e-15\n",
            " 6: -3.5338e+00 -4.0886e+00  7e-01  2e-03  3e-15\n",
            " 7: -3.6216e+00 -3.7412e+00  1e-01  2e-04  3e-15\n",
            " 8: -3.6559e+00 -3.6812e+00  3e-02  2e-05  3e-15\n",
            " 9: -3.6647e+00 -3.6694e+00  5e-03  3e-06  2e-15\n",
            "10: -3.6667e+00 -3.6668e+00  9e-05  2e-08  3e-15\n",
            "11: -3.6667e+00 -3.6668e+00  1e-06  3e-10  3e-15\n",
            "Optimal solution found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbnIdfv_gdDi",
        "outputId": "d9a5c464-4998-40e0-9062-a731867e0011"
      },
      "source": [
        "print(sol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'x': <455x1 matrix, tc='d'>, 'y': <1x1 matrix, tc='d'>, 's': <910x1 matrix, tc='d'>, 'z': <910x1 matrix, tc='d'>, 'status': 'optimal', 'gap': 1.3533037704621816e-06, 'relative gap': 3.690744858184732e-07, 'primal objective': -3.666749727933767, 'dual objective': -3.6667510615338763, 'primal infeasibility': 2.623357659985808e-10, 'dual infeasibility': 3.1923222571637324e-15, 'primal slack': 7.773534244370518e-11, 'dual slack': 1.1285744005252276e-08, 'iterations': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8iun03gjnC",
        "outputId": "6a6e2f97-9d8b-4f53-be53-5c6b30555fa8"
      },
      "source": [
        "print(sol['x'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3.34e-02]\n",
            "[ 1.52e-10]\n",
            "[ 2.72e-10]\n",
            "[ 3.23e-09]\n",
            "[ 3.44e-10]\n",
            "[ 4.12e-10]\n",
            "[ 5.55e-10]\n",
            "[ 4.80e-10]\n",
            "[ 9.41e-10]\n",
            "[ 4.09e-10]\n",
            "[ 8.84e-09]\n",
            "[ 2.34e-10]\n",
            "[ 5.98e-10]\n",
            "[ 5.63e-10]\n",
            "[ 7.28e-10]\n",
            "[ 2.42e-10]\n",
            "[ 1.83e-10]\n",
            "[ 2.73e-10]\n",
            "[ 1.25e-09]\n",
            "[ 6.58e-10]\n",
            "[ 1.71e-09]\n",
            "[ 1.11e-09]\n",
            "[ 1.00e-01]\n",
            "[ 9.50e-10]\n",
            "[ 1.00e-01]\n",
            "[ 6.69e-10]\n",
            "[ 6.51e-10]\n",
            "[ 1.81e-10]\n",
            "[ 3.41e-10]\n",
            "[ 3.54e-10]\n",
            "[ 1.06e-09]\n",
            "[ 5.85e-10]\n",
            "[ 2.91e-10]\n",
            "[ 2.78e-10]\n",
            "[ 1.00e-01]\n",
            "[ 4.75e-10]\n",
            "[ 8.88e-02]\n",
            "[ 4.10e-10]\n",
            "[ 3.01e-10]\n",
            "[ 1.53e-09]\n",
            "[ 6.53e-10]\n",
            "[ 1.10e-10]\n",
            "[ 3.91e-10]\n",
            "[ 3.75e-10]\n",
            "[ 1.26e-09]\n",
            "[ 3.24e-10]\n",
            "[ 1.75e-10]\n",
            "[ 4.78e-10]\n",
            "[ 4.81e-10]\n",
            "[ 6.91e-10]\n",
            "[ 1.28e-07]\n",
            "[ 5.42e-10]\n",
            "[ 5.87e-10]\n",
            "[ 1.00e-01]\n",
            "[ 3.43e-10]\n",
            "[ 1.34e-09]\n",
            "[ 1.55e-09]\n",
            "[ 6.36e-10]\n",
            "[ 5.77e-10]\n",
            "[ 2.02e-08]\n",
            "[ 7.27e-10]\n",
            "[ 2.35e-09]\n",
            "[ 1.00e-01]\n",
            "[ 3.33e-09]\n",
            "[ 6.78e-10]\n",
            "[ 1.67e-09]\n",
            "[ 1.23e-08]\n",
            "[ 2.62e-09]\n",
            "[ 4.68e-10]\n",
            "[ 2.22e-09]\n",
            "[ 5.02e-10]\n",
            "[ 4.72e-09]\n",
            "[ 1.00e-01]\n",
            "[ 4.93e-09]\n",
            "[ 1.84e-09]\n",
            "[ 6.17e-10]\n",
            "[ 2.32e-10]\n",
            "[ 1.07e-09]\n",
            "[ 6.62e-10]\n",
            "[ 3.58e-10]\n",
            "[ 5.21e-10]\n",
            "[ 2.83e-09]\n",
            "[ 1.00e-01]\n",
            "[ 4.15e-10]\n",
            "[ 2.57e-10]\n",
            "[ 4.10e-10]\n",
            "[ 9.47e-10]\n",
            "[ 5.06e-10]\n",
            "[ 2.20e-10]\n",
            "[ 8.11e-10]\n",
            "[ 1.96e-10]\n",
            "[ 2.37e-10]\n",
            "[ 4.25e-10]\n",
            "[ 7.38e-10]\n",
            "[ 1.00e-01]\n",
            "[ 5.59e-09]\n",
            "[ 4.10e-10]\n",
            "[ 3.23e-10]\n",
            "[ 5.73e-10]\n",
            "[ 2.79e-10]\n",
            "[ 1.09e-09]\n",
            "[ 3.88e-10]\n",
            "[ 6.46e-10]\n",
            "[ 2.75e-10]\n",
            "[ 4.28e-10]\n",
            "[ 3.09e-10]\n",
            "[ 1.21e-09]\n",
            "[ 1.16e-08]\n",
            "[ 1.95e-10]\n",
            "[ 3.25e-10]\n",
            "[ 8.34e-10]\n",
            "[ 6.08e-10]\n",
            "[ 1.70e-10]\n",
            "[ 9.30e-10]\n",
            "[ 2.24e-10]\n",
            "[ 2.37e-09]\n",
            "[ 1.43e-09]\n",
            "[ 2.71e-09]\n",
            "[ 6.96e-10]\n",
            "[ 6.36e-10]\n",
            "[ 1.00e-01]\n",
            "[ 9.92e-10]\n",
            "[ 1.03e-10]\n",
            "[ 3.76e-10]\n",
            "[ 9.89e-10]\n",
            "[ 1.90e-10]\n",
            "[ 9.84e-10]\n",
            "[ 7.53e-10]\n",
            "[ 5.33e-10]\n",
            "[ 8.01e-10]\n",
            "[ 7.99e-02]\n",
            "[ 2.57e-10]\n",
            "[ 2.87e-10]\n",
            "[ 6.01e-10]\n",
            "[ 1.00e-01]\n",
            "[ 6.42e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.34e-09]\n",
            "[ 9.62e-10]\n",
            "[ 4.14e-10]\n",
            "[ 1.42e-09]\n",
            "[ 5.53e-10]\n",
            "[ 1.09e-09]\n",
            "[ 3.61e-10]\n",
            "[ 1.00e-01]\n",
            "[ 5.28e-10]\n",
            "[ 6.00e-10]\n",
            "[ 1.89e-09]\n",
            "[ 3.66e-10]\n",
            "[ 1.00e-01]\n",
            "[ 4.46e-10]\n",
            "[ 8.72e-09]\n",
            "[ 2.34e-10]\n",
            "[ 4.80e-10]\n",
            "[ 5.28e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.00e-01]\n",
            "[ 6.60e-10]\n",
            "[ 7.55e-10]\n",
            "[ 2.48e-10]\n",
            "[ 1.00e-01]\n",
            "[ 8.40e-10]\n",
            "[ 1.73e-09]\n",
            "[ 7.99e-10]\n",
            "[ 3.52e-10]\n",
            "[ 2.12e-10]\n",
            "[ 5.88e-10]\n",
            "[ 3.97e-10]\n",
            "[ 7.34e-10]\n",
            "[ 9.28e-10]\n",
            "[ 6.81e-10]\n",
            "[ 7.01e-09]\n",
            "[ 3.67e-10]\n",
            "[ 8.37e-10]\n",
            "[ 5.01e-10]\n",
            "[ 1.74e-09]\n",
            "[ 1.46e-09]\n",
            "[ 2.68e-02]\n",
            "[ 3.19e-10]\n",
            "[ 3.49e-10]\n",
            "[ 1.00e-01]\n",
            "[ 2.06e-09]\n",
            "[ 4.92e-10]\n",
            "[ 3.02e-09]\n",
            "[ 3.83e-10]\n",
            "[ 1.00e-09]\n",
            "[ 2.51e-10]\n",
            "[ 3.46e-10]\n",
            "[ 1.31e-09]\n",
            "[ 6.75e-10]\n",
            "[ 4.28e-10]\n",
            "[ 1.04e-09]\n",
            "[ 2.89e-09]\n",
            "[ 1.00e-01]\n",
            "[ 1.45e-09]\n",
            "[ 5.59e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.00e-01]\n",
            "[ 1.07e-09]\n",
            "[ 2.58e-10]\n",
            "[ 3.82e-10]\n",
            "[ 5.65e-10]\n",
            "[ 1.00e-01]\n",
            "[ 2.10e-09]\n",
            "[ 1.18e-09]\n",
            "[ 6.73e-09]\n",
            "[ 1.02e-10]\n",
            "[ 1.50e-09]\n",
            "[ 1.35e-10]\n",
            "[ 4.38e-10]\n",
            "[ 1.75e-09]\n",
            "[ 2.46e-10]\n",
            "[ 2.56e-10]\n",
            "[ 3.51e-10]\n",
            "[ 5.89e-10]\n",
            "[ 3.58e-10]\n",
            "[ 9.22e-10]\n",
            "[ 3.09e-10]\n",
            "[ 1.34e-10]\n",
            "[ 3.25e-10]\n",
            "[ 1.44e-10]\n",
            "[ 1.47e-10]\n",
            "[ 7.13e-10]\n",
            "[ 2.66e-08]\n",
            "[ 3.70e-10]\n",
            "[ 3.49e-10]\n",
            "[ 1.62e-10]\n",
            "[ 1.23e-09]\n",
            "[ 3.99e-10]\n",
            "[ 7.04e-10]\n",
            "[ 6.87e-10]\n",
            "[ 7.75e-10]\n",
            "[ 6.49e-10]\n",
            "[ 7.32e-10]\n",
            "[ 1.76e-10]\n",
            "[ 2.68e-10]\n",
            "[ 1.00e-01]\n",
            "[ 3.09e-10]\n",
            "[ 7.37e-10]\n",
            "[ 2.58e-03]\n",
            "[ 2.04e-09]\n",
            "[ 1.19e-09]\n",
            "[ 1.03e-09]\n",
            "[ 4.29e-10]\n",
            "[ 1.84e-09]\n",
            "[ 3.76e-10]\n",
            "[ 2.83e-10]\n",
            "[ 5.17e-09]\n",
            "[ 3.65e-10]\n",
            "[ 2.64e-10]\n",
            "[ 6.19e-10]\n",
            "[ 6.87e-10]\n",
            "[ 2.51e-10]\n",
            "[ 6.19e-10]\n",
            "[ 3.09e-10]\n",
            "[ 2.61e-10]\n",
            "[ 4.66e-10]\n",
            "[ 9.94e-10]\n",
            "[ 1.84e-10]\n",
            "[ 6.41e-10]\n",
            "[ 5.46e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.51e-09]\n",
            "[ 2.11e-10]\n",
            "[ 1.93e-09]\n",
            "[ 1.14e-09]\n",
            "[ 5.49e-10]\n",
            "[ 4.85e-10]\n",
            "[ 6.05e-10]\n",
            "[ 1.58e-09]\n",
            "[ 5.49e-10]\n",
            "[ 3.33e-10]\n",
            "[ 3.07e-08]\n",
            "[ 8.22e-10]\n",
            "[ 1.84e-09]\n",
            "[ 4.01e-10]\n",
            "[ 4.24e-10]\n",
            "[ 6.05e-10]\n",
            "[ 8.64e-10]\n",
            "[ 1.00e-01]\n",
            "[ 2.71e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.04e-08]\n",
            "[ 1.00e-01]\n",
            "[ 1.26e-09]\n",
            "[ 9.78e-10]\n",
            "[ 2.54e-09]\n",
            "[ 3.47e-10]\n",
            "[ 1.95e-09]\n",
            "[ 1.00e-09]\n",
            "[ 2.18e-09]\n",
            "[ 3.76e-10]\n",
            "[ 5.45e-10]\n",
            "[ 3.82e-10]\n",
            "[ 3.01e-10]\n",
            "[ 5.89e-10]\n",
            "[ 5.26e-10]\n",
            "[ 1.04e-09]\n",
            "[ 6.33e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.24e-09]\n",
            "[ 8.25e-10]\n",
            "[ 2.88e-10]\n",
            "[ 6.52e-10]\n",
            "[ 1.06e-09]\n",
            "[ 2.11e-09]\n",
            "[ 7.66e-10]\n",
            "[ 4.91e-10]\n",
            "[ 5.08e-10]\n",
            "[ 2.54e-09]\n",
            "[ 1.00e-01]\n",
            "[ 3.70e-10]\n",
            "[ 6.51e-10]\n",
            "[ 4.31e-10]\n",
            "[ 3.73e-10]\n",
            "[ 3.06e-10]\n",
            "[ 3.22e-10]\n",
            "[ 5.21e-10]\n",
            "[ 2.79e-10]\n",
            "[ 9.30e-10]\n",
            "[ 4.29e-10]\n",
            "[ 2.04e-10]\n",
            "[ 5.28e-10]\n",
            "[ 8.24e-10]\n",
            "[ 1.04e-07]\n",
            "[ 3.55e-10]\n",
            "[ 2.30e-09]\n",
            "[ 5.70e-02]\n",
            "[ 5.78e-10]\n",
            "[ 1.00e-01]\n",
            "[ 5.79e-10]\n",
            "[ 1.40e-09]\n",
            "[ 1.00e-01]\n",
            "[ 4.19e-10]\n",
            "[ 2.03e-02]\n",
            "[ 1.51e-09]\n",
            "[ 2.84e-10]\n",
            "[ 2.29e-10]\n",
            "[ 1.01e-09]\n",
            "[ 3.83e-10]\n",
            "[ 4.35e-09]\n",
            "[ 3.64e-10]\n",
            "[ 4.96e-10]\n",
            "[ 5.35e-10]\n",
            "[ 4.81e-10]\n",
            "[ 2.29e-10]\n",
            "[ 9.82e-10]\n",
            "[ 5.18e-10]\n",
            "[ 5.24e-10]\n",
            "[ 1.87e-10]\n",
            "[ 3.26e-10]\n",
            "[ 4.26e-10]\n",
            "[ 6.73e-10]\n",
            "[ 7.17e-10]\n",
            "[ 1.00e-01]\n",
            "[ 2.65e-02]\n",
            "[ 7.07e-09]\n",
            "[ 8.68e-10]\n",
            "[ 5.59e-10]\n",
            "[ 7.25e-10]\n",
            "[ 9.03e-10]\n",
            "[ 1.97e-10]\n",
            "[ 1.74e-09]\n",
            "[ 6.05e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.19e-09]\n",
            "[ 7.88e-10]\n",
            "[ 3.05e-10]\n",
            "[ 1.38e-09]\n",
            "[ 1.47e-09]\n",
            "[ 1.13e-09]\n",
            "[ 5.57e-10]\n",
            "[ 4.09e-10]\n",
            "[ 2.75e-10]\n",
            "[ 1.14e-10]\n",
            "[ 2.27e-09]\n",
            "[ 8.47e-11]\n",
            "[ 4.96e-10]\n",
            "[ 6.27e-02]\n",
            "[ 6.64e-10]\n",
            "[ 1.00e-01]\n",
            "[ 5.92e-11]\n",
            "[ 1.00e-01]\n",
            "[ 1.68e-09]\n",
            "[ 4.05e-10]\n",
            "[ 4.67e-10]\n",
            "[ 5.02e-10]\n",
            "[ 1.12e-09]\n",
            "[ 2.78e-09]\n",
            "[ 8.76e-10]\n",
            "[ 5.39e-10]\n",
            "[ 5.42e-09]\n",
            "[ 3.61e-10]\n",
            "[ 5.27e-10]\n",
            "[ 3.33e-04]\n",
            "[ 8.75e-02]\n",
            "[ 4.33e-10]\n",
            "[ 2.54e-10]\n",
            "[ 2.17e-09]\n",
            "[ 4.69e-10]\n",
            "[ 4.37e-10]\n",
            "[ 5.98e-10]\n",
            "[ 1.58e-09]\n",
            "[ 1.00e-01]\n",
            "[ 5.90e-10]\n",
            "[ 3.57e-08]\n",
            "[ 1.21e-09]\n",
            "[ 1.07e-09]\n",
            "[ 2.20e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.24e-10]\n",
            "[ 2.81e-10]\n",
            "[ 4.17e-10]\n",
            "[ 1.00e-01]\n",
            "[ 3.60e-10]\n",
            "[ 5.33e-10]\n",
            "[ 4.31e-10]\n",
            "[ 2.98e-10]\n",
            "[ 1.56e-10]\n",
            "[ 1.17e-05]\n",
            "[ 6.98e-02]\n",
            "[ 4.30e-10]\n",
            "[ 1.33e-09]\n",
            "[ 4.29e-10]\n",
            "[ 1.00e-09]\n",
            "[ 3.93e-10]\n",
            "[ 5.13e-10]\n",
            "[ 4.21e-10]\n",
            "[ 2.19e-10]\n",
            "[ 3.08e-10]\n",
            "[ 3.21e-10]\n",
            "[ 6.09e-10]\n",
            "[ 1.87e-09]\n",
            "[ 3.36e-10]\n",
            "[ 6.65e-10]\n",
            "[ 1.00e-01]\n",
            "[ 1.00e-01]\n",
            "[ 5.00e-02]\n",
            "[ 2.69e-10]\n",
            "[ 3.65e-10]\n",
            "[ 5.77e-10]\n",
            "[ 1.00e-01]\n",
            "[ 6.08e-10]\n",
            "[ 2.69e-10]\n",
            "[ 1.66e-09]\n",
            "[ 5.17e-09]\n",
            "[ 8.23e-10]\n",
            "[ 1.00e-01]\n",
            "[ 4.48e-10]\n",
            "[ 7.66e-10]\n",
            "[ 2.09e-10]\n",
            "[ 6.73e-09]\n",
            "[ 4.56e-10]\n",
            "[ 2.91e-09]\n",
            "[ 1.03e-09]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5cqV47Fgkvz",
        "outputId": "3ac42321-5562-48c1-cc2e-d605fed90595"
      },
      "source": [
        "alphas = np.array(sol['x']) # return alphas\n",
        "\n",
        "compared_alphas = [alphas > 1e-4] \n",
        "# print(len(compared_alphas[0]))\n",
        "\n",
        "support_vector_idx = (alphas > 1e-4).flatten()\n",
        "sv = x_train[support_vector_idx]\n",
        "sv_y = y_train[support_vector_idx]\n",
        "sv_alphas = alphas[support_vector_idx]\n",
        "\n",
        "print(\"number_support_vectors:\",len(sv_alphas))\n",
        "\n",
        "\n",
        "weights = []\n",
        "for i in range(len(x_train)):\n",
        "  weights.append(x_train[i]*y_train[i]*alphas[i][0])\n",
        "\n",
        "weights = np.sum(np.array(weights),axis=0).reshape(-1,1)\n",
        "bias = sv_y.reshape(len(sv_y),1) - np.matmul(sv,weights)\n",
        "bias = bias[0]\n",
        "print(bias)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_support_vectors: 54\n",
            "[0.26901377]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiwlPhPlhkzQ",
        "outputId": "615bd27f-f958-4a22-a8ac-0f3500d420dc"
      },
      "source": [
        "pred = np.sum(linear_kernel(sv, x_train) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_train = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train, y_pred = y_pred_train))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train, y_pred = y_pred_train))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "447\n",
            "8\n",
            "confusion_matrix:\n",
            " [[163   6]\n",
            " [  2 284]]\n",
            "163 6 2 284\n",
            "training accuracy: 98.24175824175823 %\n",
            "0.9824175824175824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqQ3WnIvhv0v",
        "outputId": "cd01e6af-571a-4394-db3d-850bb4b6d0c9"
      },
      "source": [
        "pred = np.sum(linear_kernel(sv, x_test) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_test = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test, y_pred = y_pred_test))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test, y_pred = y_pred_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112\n",
            "2\n",
            "confusion_matrix:\n",
            " [[41  2]\n",
            " [ 0 71]]\n",
            "41 2 0 71\n",
            "testing accuracy: 98.24561403508771 %\n",
            "0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvZAMUhhyTN",
        "outputId": "0830ce90-3f8a-4607-91a7-d9ef188792bd"
      },
      "source": [
        "max_margin_distance = 2 / np.linalg.norm(weights)\n",
        "\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"support_vectors:\",sv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_margin_distance 1.3873543198373315\n",
            "postive_margin_distance, negative_margin_distance: 0.6936771599186657\n",
            "support_vectors: [[-1.44798723 -0.45602336 -1.36665103 ...  2.14719008  1.85943247\n",
            "   1.        ]\n",
            " [-0.17531962 -0.09300089 -0.15936626 ... -0.37978324 -0.39266635\n",
            "   1.        ]\n",
            " [ 0.14561642 -0.94238039  0.15656258 ...  0.40484036  0.04345613\n",
            "   1.        ]\n",
            " ...\n",
            " [ 1.05730199 -1.41012088  0.93217405 ... -1.80828352 -1.39846344\n",
            "   1.        ]\n",
            " [ 0.0831333   0.11178102  0.10342722 ... -0.29565865  0.53111532\n",
            "   1.        ]\n",
            " [-0.28040487  0.33750653 -0.24668948 ... -0.54641464 -0.12223716\n",
            "   1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiyjAmU_h0-3",
        "outputId": "d27eace4-2604-40ff-9173-67bc402cc6fe"
      },
      "source": [
        "compared_alphas = np.array(compared_alphas[0])\n",
        "\n",
        "sv_indexes = [i for i in range(len(compared_alphas)) if compared_alphas[i]==True]\n",
        "print(len(sv_indexes))\n",
        "# print(sv_indexes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3LoK8Ti1TP"
      },
      "source": [
        "**Regularization parameter = 0.01**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MsNtSUyjX4Q",
        "outputId": "d7dd288b-e013-4318-f7dc-653be14c9b73"
      },
      "source": [
        "C_regularization_parameter = 0.01\n",
        "phi = linear_kernel(y_train.reshape(-1,1)*x_train,y_train.reshape(-1,1)*x_train)  # kernel\n",
        "P = matrix(phi)\n",
        "q = matrix(-np.ones(len(x_train)).reshape(len(x_train),1)) # coefficients of alpha_i \n",
        "h = matrix(np.hstack((np.zeros(len(x_train)),np.ones(len(x_train))*C_regularization_parameter))) # RHS of 0 <= alpha <= C\n",
        "g = np.vstack((-np.identity(len(x_train)),np.identity(len(x_train)))) # LHS of  0 <= alpha <= C\n",
        "G = matrix(g)\n",
        "A = matrix(y_train.reshape(1,-1).astype(float))\n",
        "b = matrix(np.zeros(1))\n",
        "\n",
        "sol=solvers.qp(P=P,q=q,G=G,h=h,A=A,b=b)\n",
        "print(sol)\n",
        "print(sol['x'])\n",
        "\n",
        "\n",
        "alphas = np.array(sol['x']) # return alphas\n",
        "\n",
        "compared_alphas = [alphas > 1e-4] \n",
        "\n",
        "# print(len(compared_alphas[0])) \n",
        "\n",
        "\n",
        "support_vector_idx = (alphas > 1e-4).flatten()\n",
        "sv = x_train[support_vector_idx]\n",
        "sv_y = y_train[support_vector_idx]\n",
        "sv_alphas = alphas[support_vector_idx]\n",
        "print(\"number_support_vectors:\",len(sv_alphas))\n",
        "\n",
        "weights = []\n",
        "for i in range(len(x_train)):\n",
        "  weights.append(x_train[i]*y_train[i]*alphas[i][0])\n",
        "\n",
        "weights = np.sum(np.array(weights),axis=0).reshape(-1,1)\n",
        "bias = sv_y.reshape(len(sv_y),1) - np.matmul(sv,weights)\n",
        "bias = bias[0]\n",
        "print(bias)\n",
        "\n",
        "### metrics on training set ###\n",
        "print(\"\\nmetrics on train set\\n\")\n",
        "\n",
        "pred = np.sum(linear_kernel(sv, x_train) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_train = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train, y_pred = y_pred_train))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train, y_pred = y_pred_train))\n",
        "\n",
        "### metrics on test set ###\n",
        "print(\"\\nmetrics on test set\\n\")\n",
        "\n",
        "pred = np.sum(linear_kernel(sv, x_test) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_test = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test, y_pred = y_pred_test))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test, y_pred = y_pred_test))\n",
        "\n",
        "### margin ###\n",
        "max_margin_distance = 2 / np.linalg.norm(weights)\n",
        "\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"support_vectors:\",sv)\n",
        "\n",
        "compared_alphas = np.array(compared_alphas[0])\n",
        "\n",
        "sv_indexes = [i for i in range(len(compared_alphas)) if compared_alphas[i]==True]\n",
        "print(len(sv_indexes))\n",
        "# print(sv_indexes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -4.8903e+01 -9.1874e+00  3e+03  5e+01  5e-14\n",
            " 1: -3.3032e+00 -8.8422e+00  1e+02  2e+00  4e-14\n",
            " 2: -1.0947e+00 -7.7578e+00  2e+01  3e-01  7e-15\n",
            " 3: -6.6543e-01 -4.5835e+00  6e+00  6e-02  2e-15\n",
            " 4: -5.5197e-01 -1.2754e+00  9e-01  6e-03  1e-15\n",
            " 5: -6.7320e-01 -9.1670e-01  3e-01  2e-03  8e-16\n",
            " 6: -7.1809e-01 -8.2904e-01  1e-01  5e-04  7e-16\n",
            " 7: -7.3526e-01 -7.9927e-01  7e-02  2e-04  7e-16\n",
            " 8: -7.4769e-01 -7.7821e-01  3e-02  1e-04  6e-16\n",
            " 9: -7.5631e-01 -7.6455e-01  8e-03  9e-06  8e-16\n",
            "10: -7.5938e-01 -7.6072e-01  1e-03  7e-07  9e-16\n",
            "11: -7.5999e-01 -7.6003e-01  3e-05  1e-08  9e-16\n",
            "12: -7.6001e-01 -7.6001e-01  5e-07  2e-10  1e-15\n",
            "Optimal solution found.\n",
            "{'x': <455x1 matrix, tc='d'>, 'y': <1x1 matrix, tc='d'>, 's': <910x1 matrix, tc='d'>, 'z': <910x1 matrix, tc='d'>, 'status': 'optimal', 'gap': 4.920157106883492e-07, 'relative gap': 6.473803924738639e-07, 'primal objective': -0.7600102141002254, 'dual objective': -0.7600107032177866, 'primal infeasibility': 1.907907238049887e-10, 'dual infeasibility': 9.760189054704477e-16, 'primal slack': 6.286792466748824e-11, 'dual slack': 2.991075630890748e-08, 'iterations': 12}\n",
            "[ 6.88e-03]\n",
            "[ 1.23e-10]\n",
            "[ 2.30e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.53e-10]\n",
            "[ 3.86e-10]\n",
            "[ 6.95e-10]\n",
            "[ 4.40e-10]\n",
            "[ 9.94e-10]\n",
            "[ 5.11e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.51e-10]\n",
            "[ 8.20e-10]\n",
            "[ 4.97e-10]\n",
            "[ 6.81e-10]\n",
            "[ 2.19e-10]\n",
            "[ 1.64e-10]\n",
            "[ 2.41e-10]\n",
            "[ 1.16e-09]\n",
            "[ 6.21e-10]\n",
            "[ 1.92e-09]\n",
            "[ 2.20e-09]\n",
            "[ 1.00e-02]\n",
            "[ 9.30e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.52e-09]\n",
            "[ 6.87e-10]\n",
            "[ 1.84e-10]\n",
            "[ 2.96e-10]\n",
            "[ 2.64e-10]\n",
            "[ 1.60e-04]\n",
            "[ 6.00e-10]\n",
            "[ 3.17e-10]\n",
            "[ 3.43e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.86e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.15e-10]\n",
            "[ 2.38e-10]\n",
            "[ 4.15e-09]\n",
            "[ 5.61e-10]\n",
            "[ 9.45e-11]\n",
            "[ 4.71e-10]\n",
            "[ 2.54e-10]\n",
            "[ 1.55e-09]\n",
            "[ 2.48e-10]\n",
            "[ 1.53e-10]\n",
            "[ 4.42e-10]\n",
            "[ 3.83e-10]\n",
            "[ 5.94e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.49e-10]\n",
            "[ 5.05e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.19e-10]\n",
            "[ 6.81e-09]\n",
            "[ 7.05e-03]\n",
            "[ 5.76e-10]\n",
            "[ 4.85e-10]\n",
            "[ 1.00e-02]\n",
            "[ 6.47e-10]\n",
            "[ 3.06e-08]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 6.65e-10]\n",
            "[ 4.19e-09]\n",
            "[ 1.00e-02]\n",
            "[ 3.52e-09]\n",
            "[ 4.25e-10]\n",
            "[ 1.00e-02]\n",
            "[ 9.76e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 5.70e-10]\n",
            "[ 2.34e-10]\n",
            "[ 1.29e-09]\n",
            "[ 6.35e-10]\n",
            "[ 2.83e-10]\n",
            "[ 4.95e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 3.05e-10]\n",
            "[ 2.49e-10]\n",
            "[ 3.22e-10]\n",
            "[ 7.54e-10]\n",
            "[ 4.61e-10]\n",
            "[ 1.83e-10]\n",
            "[ 9.07e-10]\n",
            "[ 1.91e-10]\n",
            "[ 2.51e-10]\n",
            "[ 3.60e-10]\n",
            "[ 6.46e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 3.62e-10]\n",
            "[ 2.56e-10]\n",
            "[ 6.54e-10]\n",
            "[ 2.40e-10]\n",
            "[ 4.42e-09]\n",
            "[ 3.78e-10]\n",
            "[ 6.84e-10]\n",
            "[ 2.23e-10]\n",
            "[ 3.72e-10]\n",
            "[ 2.43e-10]\n",
            "[ 1.00e-08]\n",
            "[ 1.00e-02]\n",
            "[ 1.89e-10]\n",
            "[ 3.37e-10]\n",
            "[ 1.11e-09]\n",
            "[ 6.03e-10]\n",
            "[ 1.64e-10]\n",
            "[ 8.67e-10]\n",
            "[ 1.80e-10]\n",
            "[ 3.29e-09]\n",
            "[ 7.59e-09]\n",
            "[ 5.93e-03]\n",
            "[ 7.48e-10]\n",
            "[ 6.12e-10]\n",
            "[ 1.00e-02]\n",
            "[ 6.41e-03]\n",
            "[ 9.18e-11]\n",
            "[ 3.38e-10]\n",
            "[ 7.00e-10]\n",
            "[ 1.87e-10]\n",
            "[ 1.32e-09]\n",
            "[ 6.55e-10]\n",
            "[ 1.44e-09]\n",
            "[ 6.66e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.90e-10]\n",
            "[ 2.82e-10]\n",
            "[ 6.06e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.31e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.20e-09]\n",
            "[ 3.13e-10]\n",
            "[ 1.93e-09]\n",
            "[ 3.89e-10]\n",
            "[ 2.18e-09]\n",
            "[ 5.47e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.37e-10]\n",
            "[ 4.77e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.13e-10]\n",
            "[ 1.00e-02]\n",
            "[ 9.01e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.85e-10]\n",
            "[ 4.25e-10]\n",
            "[ 4.67e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 4.82e-10]\n",
            "[ 8.23e-10]\n",
            "[ 2.96e-10]\n",
            "[ 1.00e-02]\n",
            "[ 7.47e-10]\n",
            "[ 1.00e-02]\n",
            "[ 4.80e-10]\n",
            "[ 2.90e-10]\n",
            "[ 2.13e-10]\n",
            "[ 4.75e-10]\n",
            "[ 3.31e-10]\n",
            "[ 8.46e-10]\n",
            "[ 9.55e-10]\n",
            "[ 6.82e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.03e-10]\n",
            "[ 4.08e-03]\n",
            "[ 4.30e-10]\n",
            "[ 2.95e-09]\n",
            "[ 1.98e-09]\n",
            "[ 1.00e-02]\n",
            "[ 2.44e-10]\n",
            "[ 3.84e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.27e-09]\n",
            "[ 4.79e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.68e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.53e-10]\n",
            "[ 2.66e-10]\n",
            "[ 1.47e-09]\n",
            "[ 1.41e-09]\n",
            "[ 3.23e-10]\n",
            "[ 6.19e-03]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 3.59e-03]\n",
            "[ 5.93e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 2.01e-09]\n",
            "[ 1.90e-10]\n",
            "[ 4.05e-10]\n",
            "[ 5.95e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.30e-09]\n",
            "[ 2.44e-09]\n",
            "[ 3.06e-07]\n",
            "[ 9.05e-11]\n",
            "[ 3.95e-09]\n",
            "[ 1.24e-10]\n",
            "[ 3.98e-10]\n",
            "[ 3.70e-09]\n",
            "[ 2.45e-10]\n",
            "[ 2.60e-10]\n",
            "[ 2.88e-10]\n",
            "[ 5.22e-10]\n",
            "[ 4.46e-10]\n",
            "[ 9.50e-10]\n",
            "[ 3.46e-10]\n",
            "[ 1.14e-10]\n",
            "[ 2.53e-10]\n",
            "[ 1.29e-10]\n",
            "[ 1.29e-10]\n",
            "[ 5.82e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.22e-10]\n",
            "[ 4.23e-10]\n",
            "[ 1.66e-10]\n",
            "[ 2.47e-09]\n",
            "[ 4.91e-10]\n",
            "[ 6.79e-10]\n",
            "[ 6.68e-10]\n",
            "[ 1.62e-09]\n",
            "[ 6.55e-10]\n",
            "[ 1.41e-09]\n",
            "[ 1.71e-10]\n",
            "[ 2.66e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.33e-10]\n",
            "[ 8.73e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.38e-09]\n",
            "[ 1.38e-09]\n",
            "[ 3.89e-10]\n",
            "[ 3.39e-09]\n",
            "[ 5.69e-10]\n",
            "[ 3.32e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.76e-10]\n",
            "[ 3.30e-10]\n",
            "[ 6.43e-10]\n",
            "[ 6.07e-10]\n",
            "[ 2.31e-10]\n",
            "[ 7.90e-10]\n",
            "[ 2.62e-10]\n",
            "[ 2.99e-10]\n",
            "[ 4.10e-10]\n",
            "[ 2.80e-09]\n",
            "[ 1.80e-10]\n",
            "[ 1.43e-09]\n",
            "[ 4.78e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.37e-09]\n",
            "[ 2.14e-10]\n",
            "[ 2.53e-09]\n",
            "[ 1.84e-09]\n",
            "[ 5.11e-10]\n",
            "[ 3.78e-10]\n",
            "[ 5.66e-10]\n",
            "[ 1.36e-09]\n",
            "[ 1.07e-09]\n",
            "[ 2.61e-10]\n",
            "[ 1.00e-02]\n",
            "[ 6.59e-04]\n",
            "[ 3.22e-09]\n",
            "[ 3.41e-10]\n",
            "[ 3.57e-10]\n",
            "[ 7.87e-10]\n",
            "[ 8.37e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.35e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 2.92e-09]\n",
            "[ 3.10e-09]\n",
            "[ 1.00e-02]\n",
            "[ 4.65e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 8.12e-09]\n",
            "[ 2.75e-10]\n",
            "[ 6.32e-10]\n",
            "[ 4.70e-10]\n",
            "[ 2.90e-10]\n",
            "[ 5.57e-10]\n",
            "[ 5.88e-10]\n",
            "[ 1.41e-09]\n",
            "[ 6.31e-10]\n",
            "[ 1.00e-02]\n",
            "[ 4.04e-09]\n",
            "[ 9.85e-10]\n",
            "[ 3.05e-10]\n",
            "[ 5.35e-10]\n",
            "[ 1.07e-09]\n",
            "[ 4.05e-03]\n",
            "[ 8.87e-10]\n",
            "[ 3.79e-10]\n",
            "[ 3.68e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 3.19e-10]\n",
            "[ 8.40e-10]\n",
            "[ 4.06e-10]\n",
            "[ 4.54e-10]\n",
            "[ 3.28e-10]\n",
            "[ 2.38e-10]\n",
            "[ 4.48e-10]\n",
            "[ 3.02e-10]\n",
            "[ 1.71e-09]\n",
            "[ 9.24e-10]\n",
            "[ 1.89e-10]\n",
            "[ 5.19e-10]\n",
            "[ 1.26e-09]\n",
            "[ 1.00e-02]\n",
            "[ 3.17e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 4.35e-10]\n",
            "[ 1.00e-02]\n",
            "[ 4.88e-10]\n",
            "[ 2.26e-09]\n",
            "[ 1.00e-02]\n",
            "[ 4.53e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 2.88e-10]\n",
            "[ 2.64e-10]\n",
            "[ 1.12e-09]\n",
            "[ 2.89e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.49e-10]\n",
            "[ 1.32e-09]\n",
            "[ 5.42e-10]\n",
            "[ 5.61e-10]\n",
            "[ 2.25e-10]\n",
            "[ 1.52e-09]\n",
            "[ 4.87e-10]\n",
            "[ 4.44e-10]\n",
            "[ 1.78e-10]\n",
            "[ 2.85e-10]\n",
            "[ 6.15e-10]\n",
            "[ 5.53e-08]\n",
            "[ 1.23e-09]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 6.55e-08]\n",
            "[ 1.12e-09]\n",
            "[ 4.98e-10]\n",
            "[ 7.37e-10]\n",
            "[ 3.27e-09]\n",
            "[ 1.97e-10]\n",
            "[ 2.21e-09]\n",
            "[ 1.04e-09]\n",
            "[ 1.00e-02]\n",
            "[ 1.10e-09]\n",
            "[ 6.49e-10]\n",
            "[ 2.32e-10]\n",
            "[ 3.94e-09]\n",
            "[ 2.88e-09]\n",
            "[ 1.85e-09]\n",
            "[ 4.45e-10]\n",
            "[ 3.30e-10]\n",
            "[ 2.78e-10]\n",
            "[ 1.07e-10]\n",
            "[ 1.00e-02]\n",
            "[ 9.11e-11]\n",
            "[ 4.44e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.72e-10]\n",
            "[ 1.00e-02]\n",
            "[ 5.65e-11]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 3.51e-10]\n",
            "[ 3.43e-10]\n",
            "[ 5.37e-10]\n",
            "[ 1.42e-09]\n",
            "[ 5.11e-08]\n",
            "[ 1.16e-08]\n",
            "[ 3.39e-09]\n",
            "[ 1.00e-02]\n",
            "[ 2.97e-10]\n",
            "[ 4.50e-10]\n",
            "[ 1.87e-09]\n",
            "[ 1.00e-02]\n",
            "[ 3.34e-10]\n",
            "[ 2.30e-10]\n",
            "[ 1.00e-02]\n",
            "[ 7.83e-10]\n",
            "[ 7.58e-10]\n",
            "[ 5.90e-10]\n",
            "[ 4.67e-09]\n",
            "[ 1.00e-02]\n",
            "[ 7.38e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.48e-09]\n",
            "[ 1.12e-09]\n",
            "[ 2.22e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.18e-10]\n",
            "[ 2.48e-10]\n",
            "[ 3.26e-10]\n",
            "[ 1.00e-02]\n",
            "[ 2.79e-10]\n",
            "[ 4.19e-10]\n",
            "[ 3.45e-10]\n",
            "[ 2.33e-10]\n",
            "[ 1.51e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 7.01e-10]\n",
            "[ 1.66e-09]\n",
            "[ 3.74e-10]\n",
            "[ 1.22e-09]\n",
            "[ 3.31e-10]\n",
            "[ 4.31e-10]\n",
            "[ 4.67e-10]\n",
            "[ 2.06e-10]\n",
            "[ 2.42e-10]\n",
            "[ 2.69e-10]\n",
            "[ 5.03e-10]\n",
            "[ 2.21e-08]\n",
            "[ 2.92e-10]\n",
            "[ 5.40e-10]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 1.00e-02]\n",
            "[ 2.67e-10]\n",
            "[ 2.78e-10]\n",
            "[ 7.43e-10]\n",
            "[ 1.00e-02]\n",
            "[ 9.11e-10]\n",
            "[ 2.97e-10]\n",
            "[ 2.14e-09]\n",
            "[ 1.00e-02]\n",
            "[ 1.88e-09]\n",
            "[ 1.00e-02]\n",
            "[ 7.32e-10]\n",
            "[ 1.34e-09]\n",
            "[ 1.79e-10]\n",
            "[ 1.00e-02]\n",
            "[ 3.42e-10]\n",
            "[ 1.00e-02]\n",
            "[ 8.86e-10]\n",
            "\n",
            "number_support_vectors: 105\n",
            "[0.35749364]\n",
            "\n",
            "metrics on train set\n",
            "\n",
            "440\n",
            "15\n",
            "confusion_matrix:\n",
            " [[155  14]\n",
            " [  1 285]]\n",
            "155 14 1 285\n",
            "training accuracy: 96.7032967032967 %\n",
            "0.967032967032967\n",
            "\n",
            "metrics on test set\n",
            "\n",
            "111\n",
            "3\n",
            "confusion_matrix:\n",
            " [[40  3]\n",
            " [ 0 71]]\n",
            "40 3 0 71\n",
            "testing accuracy: 97.36842105263158 %\n",
            "0.9736842105263158\n",
            "max_margin_distance 2.9174455954450726\n",
            "postive_margin_distance, negative_margin_distance: 1.4587227977225363\n",
            "support_vectors: [[-1.44798723 -0.45602336 -1.36665103 ...  2.14719008  1.85943247\n",
            "   1.        ]\n",
            " [-0.98760022  1.3800326  -0.98687738 ...  0.82222776 -0.13719944\n",
            "   1.        ]\n",
            " [-0.50193594  0.58417564 -0.50206873 ...  0.11525763  0.48013277\n",
            "   1.        ]\n",
            " ...\n",
            " [-0.28040487  0.33750653 -0.24668948 ... -0.54641464 -0.12223716\n",
            "   1.        ]\n",
            " [-0.70642616 -0.22331665 -0.69195555 ... -0.15329395  0.38925083\n",
            "   1.        ]\n",
            " [-0.04183295  0.07687501 -0.03497186 ...  0.45013821  1.19444266\n",
            "   1.        ]]\n",
            "105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8GB4Tfjmk7e"
      },
      "source": [
        "**Regularization parameter = 0.001**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyyGqrqrl1k8",
        "outputId": "5022c1e4-36cc-4848-8538-f14633e1bc6c"
      },
      "source": [
        "C_regularization_parameter = 0.001\n",
        "phi = linear_kernel(y_train.reshape(-1,1)*x_train,y_train.reshape(-1,1)*x_train)  # kernel\n",
        "P = matrix(phi)\n",
        "q = matrix(-np.ones(len(x_train)).reshape(len(x_train),1)) # coefficients of alpha_i \n",
        "h = matrix(np.hstack((np.zeros(len(x_train)),np.ones(len(x_train))*C_regularization_parameter))) # RHS of 0 <= alpha <= C\n",
        "g = np.vstack((-np.identity(len(x_train)),np.identity(len(x_train)))) # LHS of  0 <= alpha <= C\n",
        "G = matrix(g)\n",
        "A = matrix(y_train.reshape(1,-1).astype(float))\n",
        "b = matrix(np.zeros(1))\n",
        "\n",
        "sol=solvers.qp(P=P,q=q,G=G,h=h,A=A,b=b)\n",
        "print(sol)\n",
        "print(sol['x'])\n",
        "\n",
        "\n",
        "alphas = np.array(sol['x']) # return alphas\n",
        "\n",
        "compared_alphas = [alphas > 1e-4] \n",
        "\n",
        "# print(len(compared_alphas[0])) \n",
        "\n",
        "\n",
        "support_vector_idx = (alphas > 1e-4).flatten()\n",
        "sv = x_train[support_vector_idx]\n",
        "sv_y = y_train[support_vector_idx]\n",
        "sv_alphas = alphas[support_vector_idx]\n",
        "print(\"number_support_vectors:\",len(sv_alphas))\n",
        "\n",
        "weights = []\n",
        "for i in range(len(x_train)):\n",
        "  weights.append(x_train[i]*y_train[i]*alphas[i][0])\n",
        "\n",
        "weights = np.sum(np.array(weights),axis=0).reshape(-1,1)\n",
        "bias = sv_y.reshape(len(sv_y),1) - np.matmul(sv,weights)\n",
        "bias = bias[0]\n",
        "print(bias)\n",
        "\n",
        "### metrics on training set ###\n",
        "print(\"\\nmetrics on train set\\n\")\n",
        "\n",
        "pred = np.sum(linear_kernel(sv, x_train) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_train = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train, y_pred = y_pred_train))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train, y_pred = y_pred_train))\n",
        "\n",
        "### metrics on test set ###\n",
        "print(\"\\nmetrics on test set\\n\")\n",
        "\n",
        "pred = np.sum(linear_kernel(sv, x_test) * sv_alphas * sv_y.reshape(sv_y.shape[0],1), axis=0) + bias\n",
        "y_pred_test = np.sign(pred)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test, y_pred = y_pred_test))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test, y_pred = y_pred_test))\n",
        "\n",
        "### margin ###\n",
        "max_margin_distance = 2 / np.linalg.norm(weights)\n",
        "\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"support_vectors:\",sv)\n",
        "\n",
        "compared_alphas = np.array(compared_alphas[0])\n",
        "\n",
        "sv_indexes = [i for i in range(len(compared_alphas)) if compared_alphas[i]==True]\n",
        "print(len(sv_indexes))\n",
        "# print(sv_indexes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -4.8473e+01 -1.4801e+00  3e+03  5e+01  5e-14\n",
            " 1: -2.5856e+00 -1.1791e+00  1e+02  2e+00  5e-14\n",
            " 2: -2.6599e-01 -9.2883e-01  8e+00  1e-01  4e-15\n",
            " 3: -1.2861e-01 -7.7032e-01  1e+00  1e-02  8e-16\n",
            " 4: -1.2369e-01 -2.3348e-01  1e-01  1e-04  9e-16\n",
            " 5: -1.5002e-01 -1.7863e-01  3e-02  2e-05  4e-16\n",
            " 6: -1.5794e-01 -1.6821e-01  1e-02  5e-06  4e-16\n",
            " 7: -1.6066e-01 -1.6448e-01  4e-03  1e-06  4e-16\n",
            " 8: -1.6183e-01 -1.6294e-01  1e-03  4e-07  4e-16\n",
            " 9: -1.6219e-01 -1.6249e-01  3e-04  8e-08  4e-16\n",
            "10: -1.6231e-01 -1.6234e-01  3e-05  2e-09  5e-16\n",
            "11: -1.6232e-01 -1.6233e-01  5e-06  2e-10  4e-16\n",
            "12: -1.6232e-01 -1.6232e-01  8e-08  3e-12  5e-16\n",
            "Optimal solution found.\n",
            "{'x': <455x1 matrix, tc='d'>, 'y': <1x1 matrix, tc='d'>, 's': <910x1 matrix, tc='d'>, 'z': <910x1 matrix, tc='d'>, 'status': 'optimal', 'gap': 7.849174387288121e-08, 'relative gap': 4.835473698487387e-07, 'primal objective': -0.1623248284804748, 'dual objective': -0.1623249069488046, 'primal infeasibility': 3.3193513661608328e-12, 'dual infeasibility': 4.882553485629539e-16, 'primal slack': 2.182166443074185e-11, 'dual slack': 7.344166370034346e-08, 'iterations': 12}\n",
            "[ 1.00e-03]\n",
            "[ 5.57e-11]\n",
            "[ 1.55e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.55e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.72e-10]\n",
            "[ 2.17e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 3.02e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.88e-10]\n",
            "[ 5.21e-10]\n",
            "[ 2.08e-10]\n",
            "[ 6.81e-11]\n",
            "[ 2.40e-10]\n",
            "[ 3.11e-08]\n",
            "[ 3.25e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 5.66e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 4.48e-10]\n",
            "[ 1.58e-10]\n",
            "[ 1.60e-10]\n",
            "[ 1.34e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.91e-09]\n",
            "[ 1.00e-03]\n",
            "[ 4.87e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.31e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.75e-10]\n",
            "[ 9.88e-11]\n",
            "[ 1.00e-03]\n",
            "[ 2.08e-10]\n",
            "[ 3.44e-11]\n",
            "[ 1.00e-03]\n",
            "[ 8.73e-11]\n",
            "[ 1.00e-03]\n",
            "[ 1.09e-10]\n",
            "[ 7.00e-11]\n",
            "[ 4.10e-10]\n",
            "[ 1.52e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.53e-09]\n",
            "[ 2.99e-10]\n",
            "[ 1.00e-03]\n",
            "[ 7.87e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 4.98e-10]\n",
            "[ 5.82e-10]\n",
            "[ 1.00e-03]\n",
            "[ 6.16e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 5.11e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.40e-09]\n",
            "[ 2.48e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.43e-09]\n",
            "[ 2.52e-10]\n",
            "[ 1.00e-03]\n",
            "[ 7.83e-09]\n",
            "[ 1.13e-10]\n",
            "[ 3.09e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.27e-10]\n",
            "[ 1.51e-10]\n",
            "[ 1.60e-10]\n",
            "[ 6.53e-10]\n",
            "[ 2.96e-10]\n",
            "[ 7.84e-11]\n",
            "[ 5.54e-09]\n",
            "[ 1.98e-10]\n",
            "[ 2.68e-10]\n",
            "[ 2.13e-10]\n",
            "[ 3.14e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.11e-10]\n",
            "[ 1.19e-10]\n",
            "[ 1.05e-09]\n",
            "[ 1.09e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.41e-10]\n",
            "[ 5.47e-10]\n",
            "[ 1.14e-10]\n",
            "[ 2.28e-10]\n",
            "[ 1.25e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.86e-10]\n",
            "[ 2.12e-09]\n",
            "[ 1.00e-03]\n",
            "[ 2.77e-10]\n",
            "[ 8.21e-11]\n",
            "[ 7.02e-10]\n",
            "[ 8.77e-11]\n",
            "[ 3.02e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.81e-10]\n",
            "[ 4.24e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 4.01e-11]\n",
            "[ 2.72e-10]\n",
            "[ 8.66e-10]\n",
            "[ 1.84e-10]\n",
            "[ 1.00e-03]\n",
            "[ 4.53e-10]\n",
            "[ 1.00e-03]\n",
            "[ 4.00e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.12e-10]\n",
            "[ 2.50e-10]\n",
            "[ 1.30e-09]\n",
            "[ 1.00e-03]\n",
            "[ 2.58e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.35e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.36e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.30e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.73e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 7.53e-11]\n",
            "[ 6.34e-10]\n",
            "[ 5.76e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.23e-10]\n",
            "[ 1.00e-03]\n",
            "[ 5.03e-10]\n",
            "[ 1.00e-03]\n",
            "[ 4.63e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 3.15e-10]\n",
            "[ 1.33e-10]\n",
            "[ 3.50e-10]\n",
            "[ 1.62e-10]\n",
            "[ 8.42e-10]\n",
            "[ 2.35e-09]\n",
            "[ 7.45e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.30e-10]\n",
            "[ 1.00e-03]\n",
            "[ 5.11e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.13e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.56e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.46e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.45e-10]\n",
            "[ 1.14e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.41e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.41e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 7.78e-11]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 6.26e-08]\n",
            "[ 3.61e-11]\n",
            "[ 1.00e-03]\n",
            "[ 5.58e-11]\n",
            "[ 2.99e-10]\n",
            "[ 1.00e-03]\n",
            "[ 5.16e-09]\n",
            "[ 2.96e-10]\n",
            "[ 1.18e-10]\n",
            "[ 3.42e-10]\n",
            "[ 1.00e-03]\n",
            "[ 7.63e-10]\n",
            "[ 7.73e-10]\n",
            "[ 4.15e-11]\n",
            "[ 1.61e-10]\n",
            "[ 5.92e-11]\n",
            "[ 7.17e-11]\n",
            "[ 3.49e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.17e-10]\n",
            "[ 1.00e-03]\n",
            "[ 9.15e-10]\n",
            "[ 5.28e-10]\n",
            "[ 5.36e-09]\n",
            "[ 1.00e-03]\n",
            "[ 9.73e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.86e-10]\n",
            "[ 3.48e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.43e-10]\n",
            "[ 4.10e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 9.44e-08]\n",
            "[ 1.95e-04]\n",
            "[ 2.49e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.50e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.33e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.90e-10]\n",
            "[ 4.71e-10]\n",
            "[ 1.01e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.22e-10]\n",
            "[ 3.28e-04]\n",
            "[ 2.14e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.22e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.29e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.08e-10]\n",
            "[ 3.95e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.79e-10]\n",
            "[ 1.86e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.83e-05]\n",
            "[ 1.00e-03]\n",
            "[ 1.13e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.60e-10]\n",
            "[ 1.83e-10]\n",
            "[ 1.00e-03]\n",
            "[ 5.86e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.68e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.17e-10]\n",
            "[ 2.45e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.31e-09]\n",
            "[ 6.06e-10]\n",
            "[ 4.97e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.95e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.67e-09]\n",
            "[ 5.50e-10]\n",
            "[ 3.82e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 9.06e-04]\n",
            "[ 1.55e-10]\n",
            "[ 1.40e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.58e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.93e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.11e-09]\n",
            "[ 1.30e-10]\n",
            "[ 6.33e-10]\n",
            "[ 8.88e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.24e-10]\n",
            "[ 4.39e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.51e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.04e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.63e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.37e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 7.20e-10]\n",
            "[ 2.69e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.25e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 5.66e-10]\n",
            "[ 1.00e-03]\n",
            "[ 4.95e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 3.12e-10]\n",
            "[ 1.13e-10]\n",
            "[ 1.19e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 3.65e-09]\n",
            "[ 1.00e-03]\n",
            "[ 3.69e-10]\n",
            "[ 9.31e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.29e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 4.68e-10]\n",
            "[ 1.16e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.30e-10]\n",
            "[ 1.36e-10]\n",
            "[ 4.07e-10]\n",
            "[ 5.31e-11]\n",
            "[ 1.00e-03]\n",
            "[ 3.54e-11]\n",
            "[ 3.22e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.64e-10]\n",
            "[ 1.00e-03]\n",
            "[ 2.17e-11]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.61e-10]\n",
            "[ 1.59e-10]\n",
            "[ 1.66e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.80e-10]\n",
            "[ 2.65e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.72e-10]\n",
            "[ 1.23e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 6.24e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.17e-09]\n",
            "[ 1.97e-09]\n",
            "[ 1.61e-10]\n",
            "[ 1.00e-03]\n",
            "[ 6.34e-11]\n",
            "[ 1.47e-10]\n",
            "[ 1.49e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.19e-10]\n",
            "[ 2.77e-10]\n",
            "[ 1.48e-10]\n",
            "[ 1.10e-10]\n",
            "[ 7.16e-11]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.34e-10]\n",
            "[ 1.00e-03]\n",
            "[ 3.06e-10]\n",
            "[ 2.12e-10]\n",
            "[ 5.57e-10]\n",
            "[ 1.25e-10]\n",
            "[ 1.23e-10]\n",
            "[ 1.46e-10]\n",
            "[ 1.69e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.33e-10]\n",
            "[ 8.93e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 2.64e-10]\n",
            "[ 1.35e-10]\n",
            "[ 1.26e-07]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.61e-09]\n",
            "[ 1.04e-09]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.00e-03]\n",
            "[ 1.24e-10]\n",
            "[ 1.00e-03]\n",
            "[ 1.63e-10]\n",
            "[ 1.00e-03]\n",
            "[ 4.06e-10]\n",
            "\n",
            "number_support_vectors: 218\n",
            "[1.1176823]\n",
            "\n",
            "metrics on train set\n",
            "\n",
            "369\n",
            "86\n",
            "confusion_matrix:\n",
            " [[ 83  86]\n",
            " [  0 286]]\n",
            "83 86 0 286\n",
            "training accuracy: 81.0989010989011 %\n",
            "0.810989010989011\n",
            "\n",
            "metrics on test set\n",
            "\n",
            "93\n",
            "21\n",
            "confusion_matrix:\n",
            " [[22 21]\n",
            " [ 0 71]]\n",
            "22 21 0 71\n",
            "testing accuracy: 81.57894736842105 %\n",
            "0.8157894736842105\n",
            "max_margin_distance 6.0777800808029925\n",
            "postive_margin_distance, negative_margin_distance: 3.0388900404014962\n",
            "support_vectors: [[-1.44798723 -0.45602336 -1.36665103 ...  2.14719008  1.85943247\n",
            "   1.        ]\n",
            " [-0.98760022  1.3800326  -0.98687738 ...  0.82222776 -0.13719944\n",
            "   1.        ]\n",
            " [ 0.117215    1.91991217  0.19610517 ...  2.13101227  2.77933504\n",
            "   1.        ]\n",
            " ...\n",
            " [-0.29744572 -0.83300824 -0.26110605 ...  0.45822712 -0.11724974\n",
            "   1.        ]\n",
            " [-0.70642616 -0.22331665 -0.69195555 ... -0.15329395  0.38925083\n",
            "   1.        ]\n",
            " [-0.04183295  0.07687501 -0.03497186 ...  0.45013821  1.19444266\n",
            "   1.        ]]\n",
            "218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB20JQhcq2aR"
      },
      "source": [
        "# Soft SVM with SGD from Scratch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeFOwEgqq6EG",
        "outputId": "2f0d6c60-a90c-473e-9386-5268186a6d12"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "BreastCancer = pd.read_csv(\"/content/BreastCancer.csv\")\n",
        "BreastCancer = BreastCancer.drop(['Unnamed: 32'], axis=1)\n",
        "X1 = BreastCancer.iloc[:,2:].values\n",
        "Y = BreastCancer.iloc[:,1].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "scaler.fit(X1)\n",
        "X1 = scaler.transform(X1)\n",
        "\n",
        "\n",
        "X1 = np.hstack((X1,np.ones(len(X1)).reshape(len(X1),1)))\n",
        "Y1 = []\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == 'M': #if Y[i] == 0:#\n",
        "    Y1.append(-1)\n",
        "  elif Y[i] == 'B':#elif Y[i] == 1:#elif Y[i] == 'B':\n",
        "    Y1.append(1)\n",
        "    \n",
        "Y1 = np.array(Y1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X1,Y1,test_size = 0.2,shuffle = True,random_state = 42)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(455, 31)\n",
            "(455,)\n",
            "(114, 31)\n",
            "(114,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z2hxMg3rGWy"
      },
      "source": [
        "**Regularization parameter = 0.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VGDtj272QiG"
      },
      "source": [
        "import random\n",
        "def soft_svm_sgd(x_train,y_train,T,lambda_hyperparameter):\n",
        "  accuracies =[]\n",
        "  theta = np.array([float(0) for i in range(0,x_train.shape[1])]).astype('float')\n",
        "  positive_sv= []\n",
        "  negative_sv = []\n",
        "  positive_sv_count = 0\n",
        "  negative_sv_count = 0\n",
        "  weight_vectors = []\n",
        "  random_indexes = np.arange(0,len(x_train),1)\n",
        "\n",
        "  for t in range(1,T+1):\n",
        "\n",
        "    weights = (1.0/(lambda_hyperparameter * np.float(t))) * theta\n",
        "    weight_vectors.append(weights)\n",
        "\n",
        "    i = np.random.randint(0,len(x_train)-1) \n",
        "    if round(np.sum(x_train[i]*weights),1) == 1:\n",
        "      positive_sv_count += 1\n",
        "      positive_sv.append(x_train[i])\n",
        "    \n",
        "    if round(np.sum(x_train[i]*weights),1) == -1:\n",
        "      negative_sv_count += 1\n",
        "      negative_sv.append(x_train[i])\n",
        "\n",
        "    if y_train[i] * np.sum(x_train[i]*weights) < 1:\n",
        "      theta = theta + y_train[i] * x_train[i]\n",
        "\n",
        "    else:\n",
        "      theta = theta \n",
        "\n",
        "    if t % 1 == 0 :\n",
        "      y_pre = np.sign(np.matmul(x_train[:,:-1],weights[0:-1])+weights[-1])\n",
        "      # print(\"iteration:\",t,\"accuracy:\",accuracy_score(y_true = y_train, y_pred = y_pre)*100)\n",
        "      accuracies.append(accuracy_score(y_true = y_train, y_pred = y_pre)*100)\n",
        "\n",
        "  final_weights = np.sum(weight_vectors,axis=0) / T\n",
        "\n",
        "  return weights,accuracies,positive_sv,negative_sv,positive_sv_count,negative_sv_count\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecP8dL_KrIBk"
      },
      "source": [
        "num_iterations = 1500\n",
        "weights,accuracies,positive_sv,negative_sv,positive_sv_count,negative_sv_count = soft_svm_sgd(x_train,y_train,num_iterations,0.1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdoo69ryrKos",
        "outputId": "5e142b39-5e5a-41b8-ef1e-f87deecca0cc"
      },
      "source": [
        "print(\"number_of_positive_suuport_vectors:\",positive_sv_count)\n",
        "print(\"number_of_negative_suuport_vectors:\",negative_sv_count)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_of_positive_suuport_vectors: 23\n",
            "number_of_negative_suuport_vectors: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "5UQNrfbWrM2G",
        "outputId": "874c555e-3495-42bb-aef0-71f498ef434a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "iter = np.arange(0,num_iterations/1)\n",
        "plt.plot(iter,accuracies)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1fbd06f510>]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgl0hx3HrPPB",
        "outputId": "6364f731-7331-47a6-ec99-08883fe8c5c7"
      },
      "source": [
        "optimized_bias = weights[-1]\n",
        "optimized_weights = weights[0:-1]\n",
        "print(optimized_weights)\n",
        "print(optimized_bias)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.19568594 -0.17343886 -0.19051381 -0.2091247  -0.04721622 -0.00445959\n",
            " -0.19377543 -0.23413327 -0.0285384   0.08847818 -0.22665209  0.04546709\n",
            " -0.17028146 -0.20559263  0.03144083  0.12872729  0.05327246  0.05596103\n",
            "  0.05601305  0.13070514 -0.25128252 -0.25774316 -0.2242707  -0.24894574\n",
            " -0.19162029 -0.05683025 -0.17924163 -0.18494596 -0.25213345 -0.06823943]\n",
            "0.25333333333333335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeWkU_giriLe",
        "outputId": "9c572bab-64ee-49f2-fb86-088af7933bab"
      },
      "source": [
        "y_pred_train = np.sign(np.matmul(x_train[:,:-1],optimized_weights)+optimized_bias)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "\n",
        "y_pred_train_acc = []\n",
        "y_train_acc = []\n",
        "for i in range(len(y_pred_train)):\n",
        "\n",
        "  if y_pred_train[i] == -1:\n",
        "    y_pred_train_acc.append(-1)\n",
        "  if y_pred_train[i] == 1:\n",
        "    y_pred_train_acc.append(1)\n",
        "\n",
        "  if y_train[i] == -1:\n",
        "    y_train_acc.append(-1)\n",
        "  \n",
        "  if y_train[i] == 1:\n",
        "    y_train_acc.append(1)\n",
        "\n",
        "y_pred_train_acc = np.array(y_pred_train_acc)\n",
        "y_train_acc = np.array(y_train_acc)\n",
        "# print(y_pred_train_acc)\n",
        "# print(y_train_acc)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj4NEw3ur1Bo",
        "outputId": "903ba0a6-932c-4e33-8e33-27974a8ee705"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train_acc, y_pred_train_acc).ravel()\n",
        "training_accuracy_lp = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy_lp:\",training_accuracy_lp*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion_matrix:\n",
            " [[158  11]\n",
            " [  2 284]]\n",
            "158 11 2 284\n",
            "training accuracy_lp: 97.14285714285714 %\n",
            "0.9714285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE8uFsYGr52K",
        "outputId": "3e91844b-1df2-4e3b-8360-58d67683a89c"
      },
      "source": [
        "y_pred_test = np.sign(np.matmul(x_test[:,:-1],optimized_weights)+optimized_bias)\n",
        "# y_test_true = np.array(y_test)\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "y_pred_test_acc = []\n",
        "y_test_acc = []\n",
        "for i in range(len(y_pred_test)):\n",
        "\n",
        "  if y_pred_test[i] == -1:\n",
        "    y_pred_test_acc.append(-1)\n",
        "  if y_pred_test[i] == 1:\n",
        "    y_pred_test_acc.append(1)\n",
        "\n",
        "  if y_test[i] == -1:\n",
        "    y_test_acc.append(-1)\n",
        "  \n",
        "  if y_test[i] == 1:\n",
        "    y_test_acc.append(1)\n",
        "\n",
        "y_pred_test_acc = np.array(y_pred_test_acc)\n",
        "y_test_acc = np.array(y_test_acc)\n",
        "# print(y_pred_test_acc)\n",
        "# print(y_test_acc)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYvAr3xzsWRT",
        "outputId": "4993613b-593a-4b38-e1a7-4e7025428b27"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test_acc, y_pred = y_pred_test_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test_acc, y_pred_test_acc).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy_lp:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test_acc, y_pred = y_pred_test_acc))\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion_matrix:\n",
            " [[41  2]\n",
            " [ 0 71]]\n",
            "41 2 0 71\n",
            "testing accuracy_lp: 98.24561403508771 %\n",
            "0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT3KrcxZshOr",
        "outputId": "d76dcffc-20c5-4111-e3f5-6e25c5a865b8"
      },
      "source": [
        "def finding_margin(y_pred_train,optimized_weights,optimized_bias):\n",
        "\n",
        "  max_margin_distance = 2 / np.linalg.norm(optimized_weights)\n",
        "\n",
        "  return max_margin_distance\n",
        "\n",
        "max_margin_distance = finding_margin(y_pred_train,optimized_weights,optimized_bias)\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"positive_support_vectors:\",positive_sv)\n",
        "print(\"negative_support_vectors:\",negative_sv)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_margin_distance 2.1867799077731696\n",
            "postive_margin_distance, negative_margin_distance: 1.0933899538865848\n",
            "positive_support_vectors: [array([ 0.02065018,  0.28863812,  0.0181635 , -0.10377876, -0.50173638,\n",
            "        0.12240791, -0.47921466, -0.4730396 , -1.11579632, -0.38383187,\n",
            "       -0.2070678 , -0.3607885 , -0.0579479 , -0.20521169, -0.97803262,\n",
            "        0.41368661, -0.12214107,  0.27638303, -0.53786689, -0.3975058 ,\n",
            "        0.03744263,  0.25774538,  0.14412734, -0.09155759, -0.74821703,\n",
            "        0.56384216, -0.10069274,  0.29377857, -0.59333028, -0.29735115,\n",
            "        1.        ]), array([-0.89671568, -0.48627523, -0.83364984, -0.80598769, -0.51312285,\n",
            "        0.13188364,  0.07245077, -0.329883  , -1.17786235,  0.51067629,\n",
            "       -0.53334039,  0.28874661, -0.02973116, -0.48815367,  0.50802763,\n",
            "        1.16819539,  1.07449589,  0.95441944, -0.65168068,  0.4467512 ,\n",
            "       -0.80952492,  0.1942363 , -0.50996967, -0.71051866,  0.29507718,\n",
            "        0.97924139,  0.98690584,  0.62267307, -0.5836236 ,  0.63030964,\n",
            "        1.        ]), array([-0.11851678, -0.1418693 , -0.13341643, -0.23858922,  0.19924323,\n",
            "        0.05039228, -0.43878765, -0.28603323, -0.35640009,  0.79844991,\n",
            "       -0.31065213,  0.05832494, -0.28813713, -0.30553948, -0.10066372,\n",
            "       -0.16588791, -0.36584519,  0.27962723, -0.22306278, -0.0181197 ,\n",
            "       -0.24004796, -0.00769   , -0.23325923, -0.31410934,  0.44411921,\n",
            "        0.01485358, -0.37750985,  0.21003229, -0.08372939,  0.35267639,\n",
            "        1.        ]), array([ 1.05730199, -1.41012088,  0.93217405,  0.95906239, -1.27957466,\n",
            "       -0.79920254, -0.55680438, -0.184147  , -2.15996613, -1.46971658,\n",
            "        0.28234108, -0.30998687,  0.14699477,  0.2335024 , -0.89069575,\n",
            "       -0.9615314 , -0.67520021, -0.70709417, -0.91078868, -0.94029647,\n",
            "        0.73531076, -1.18179374,  0.59091491,  0.57908611, -1.4793997 ,\n",
            "       -0.98286795, -0.80305019, -0.47501233, -1.80828352, -1.39846344,\n",
            "        1.        ]), array([-0.19520061,  0.53298017, -0.23845144, -0.26134204, -1.04899863,\n",
            "       -0.83445229, -0.72441324, -0.7379438 , -0.10083405, -0.98206078,\n",
            "       -0.60155446, -0.70823536, -0.64059893, -0.4357896 , -1.25371047,\n",
            "       -0.80805872, -0.59661807, -0.79728274, -0.81634745, -0.94899625,\n",
            "       -0.07024028,  0.74464831, -0.14181671, -0.16292885, -1.00684879,\n",
            "       -0.31784749, -0.305547  , -0.05186519,  0.1508488 , -0.69191177,\n",
            "        1.        ]), array([-0.38549012,  2.35972786, -0.43740011, -0.41805209, -0.96787002,\n",
            "       -1.17501038, -0.86415007, -0.87516779, -0.99531518, -0.91118058,\n",
            "       -0.59289235,  0.27241752, -0.68762684, -0.47473263,  0.59803128,\n",
            "       -0.73484342, -0.61717542, -0.40408651, -0.19037158, -0.78256566,\n",
            "       -0.4968303 ,  1.68100011, -0.57073278, -0.50255759, -0.39314631,\n",
            "       -0.94062827, -0.89070094, -0.75563852, -0.79878842, -1.05876448,\n",
            "        1.        ]), array([-0.38549012,  0.74008914, -0.42215974, -0.42260266, -0.41349123,\n",
            "       -0.88562128, -0.52340816, -0.5643503 , -0.82737179, -0.30444604,\n",
            "       -0.60805103,  0.29600399, -0.69752745, -0.45031073,  0.15267989,\n",
            "       -0.75216918, -0.28427891, -0.46653723, -0.49306784, -0.4410047 ,\n",
            "       -0.36843913,  1.25272094, -0.45337658, -0.39901653,  0.41781768,\n",
            "       -0.64864014, -0.26140978, -0.32426902, -0.116085  , -0.18153209,\n",
            "        1.        ]), array([ 0.32454537, -1.48458703,  0.25541906,  0.20082463, -1.03476554,\n",
            "       -0.79673885, -0.37513395, -0.44750356, -1.652485  , -1.06853463,\n",
            "       -0.69214563, -1.54954662, -0.66386537, -0.44745051, -0.9166968 ,\n",
            "       -0.73316674, -0.44707989, -0.71747559, -1.33226295, -0.80904326,\n",
            "        0.23002939, -1.58890322,  0.19178468,  0.09161691, -0.44574938,\n",
            "       -0.22687951,  0.11519582, -0.16910999, -0.93953533, -0.51014789,\n",
            "        1.        ]), array([ 0.11437486, -1.23559085,  0.07788929, -0.03040091,  0.96356007,\n",
            "       -0.22592029, -0.24920759,  0.41349957, -0.59006047, -0.22506022,\n",
            "       -0.35793278, -0.89910434, -0.35793645, -0.32204076, -0.30267191,\n",
            "       -0.72478331, -0.52201476, -0.07074565,  0.18497178, -0.08771795,\n",
            "       -0.09716101, -1.42443099, -0.12394521, -0.22972953,  0.10219926,\n",
            "       -0.67726643, -0.64713067, -0.11733956, -0.47685008, -0.32395074,\n",
            "        1.        ]), array([-0.19520061,  0.53298017, -0.23845144, -0.26134204, -1.04899863,\n",
            "       -0.83445229, -0.72441324, -0.7379438 , -0.10083405, -0.98206078,\n",
            "       -0.60155446, -0.70823536, -0.64059893, -0.4357896 , -1.25371047,\n",
            "       -0.80805872, -0.59661807, -0.79728274, -0.81634745, -0.94899625,\n",
            "       -0.07024028,  0.74464831, -0.14181671, -0.16292885, -1.00684879,\n",
            "       -0.31784749, -0.305547  , -0.05186519,  0.1508488 , -0.69191177,\n",
            "        1.        ]), array([-0.16679919, -1.1471623 , -0.18572799, -0.2519565 ,  0.10174657,\n",
            "       -0.43685025, -0.27820957, -0.02860929,  0.26791123, -0.72830966,\n",
            "       -0.48822526, -0.77699899, -0.40001405, -0.36912442,  0.4736929 ,\n",
            "       -0.60797417, -0.26604255,  0.21960965, -0.08987642, -0.56544939,\n",
            "       -0.24004796, -1.04500496, -0.22521706, -0.29776075,  0.50987305,\n",
            "       -0.48960521, -0.15922253,  0.21612292,  0.12334653, -0.62929189,\n",
            "        1.        ]), array([ 0.2251404 , -1.01451947,  0.18457191,  0.09104227, -1.09454451,\n",
            "       -0.35725404, -0.41945299, -0.43099541, -0.45497556, -0.86581725,\n",
            "       -0.57953827, -1.42145394, -0.51486115, -0.37462485, -1.25671059,\n",
            "       -0.4347166 , -0.29157345, -0.55104847, -1.09724957, -0.53556754,\n",
            "        0.18861288, -1.2143625 ,  0.14114875,  0.04573539, -1.13397288,\n",
            "        0.15798502,  0.20586901,  0.0014279 , -0.34419207, -0.06737551,\n",
            "        1.        ]), array([-0.14407806,  0.91694624, -0.19684934, -0.23233219, -0.27756523,\n",
            "       -0.6987597 , -0.741488  , -0.6316726 , -0.53894726, -0.67869351,\n",
            "       -0.21356438,  0.21617286, -0.39605381, -0.20015129, -0.39100882,\n",
            "       -0.25084001, -0.38739725, -0.44317904,  0.03967757, -0.45840426,\n",
            "       -0.19034815,  0.55574951, -0.28836304, -0.26506358, -0.47205092,\n",
            "       -0.65245698, -0.80257043, -0.65270672, -0.41860997, -0.7988643 ,\n",
            "        1.        ]), array([-0.30596615, -0.1628129 , -0.28334876, -0.40639127,  0.84257882,\n",
            "        0.4938569 ,  0.09078103,  0.18341725,  0.81555274,  0.31362933,\n",
            "       -0.34133041,  0.5318687 , -0.13665777, -0.41290782,  0.25535072,\n",
            "        0.77361525,  0.63748633,  1.12960587,  0.97561441,  0.72627894,\n",
            "       -0.43056389, -0.13470816, -0.38844345, -0.50871028,  0.0846649 ,\n",
            "        0.07337844, -0.07142785,  0.17044313,  0.34336469, -0.04299255,\n",
            "        1.        ]), array([ 1.05730199, -1.41012088,  0.93217405,  0.95906239, -1.27957466,\n",
            "       -0.79920254, -0.55680438, -0.184147  , -2.15996613, -1.46971658,\n",
            "        0.28234108, -0.30998687,  0.14699477,  0.2335024 , -0.89069575,\n",
            "       -0.9615314 , -0.67520021, -0.70709417, -0.91078868, -0.94029647,\n",
            "        0.73531076, -1.18179374,  0.59091491,  0.57908611, -1.4793997 ,\n",
            "       -0.98286795, -0.80305019, -0.47501233, -1.80828352, -1.39846344,\n",
            "        1.        ]), array([-0.35424856, -0.24891439, -0.30971049, -0.46014481,  1.81042883,\n",
            "        1.17042472, -0.50909549,  0.1060353 , -0.37465481,  1.37966758,\n",
            "        0.13508531, -0.08682257,  0.15343017, -0.13128594, -0.5893502 ,\n",
            "       -0.12229407, -0.59164451,  0.10444079, -0.28844517, -0.1875763 ,\n",
            "       -0.25247291, -0.21287318, -0.23683354, -0.36192457,  0.58001047,\n",
            "        0.26612877, -0.70805922, -0.07622775, -0.51567681,  0.27620256,\n",
            "        1.        ]), array([-0.98760022,  1.3800326 , -0.98687738, -0.8756682 ,  0.01492473,\n",
            "       -0.60646599, -0.81619008, -0.84524677,  0.31172255,  0.06980143,\n",
            "       -0.5611313 ,  0.50102485, -0.67772623, -0.52137625,  0.04934236,\n",
            "       -0.84550471, -0.69907327, -0.90044809,  0.12564331, -0.44478722,\n",
            "       -0.83230399,  1.54909664, -0.87216547, -0.74690745,  0.76850481,\n",
            "       -0.72815761, -0.76610926, -0.8107588 ,  0.82222776, -0.13719944,\n",
            "        1.        ]), array([-0.35424856,  2.24104744, -0.39003138, -0.39984984, -1.07675315,\n",
            "       -0.87368185, -0.33709238, -0.65746658, -0.89673971, -0.81053069,\n",
            "       -0.6986422 ,  0.25971711, -0.67525107, -0.51719593,  0.45702556,\n",
            "       -0.22065966,  0.23496007, -0.6774098 , -0.43495016, -0.37594547,\n",
            "       -0.49268864,  1.63866073, -0.54869126, -0.50079967, -0.42383144,\n",
            "       -0.58693459, -0.13571466, -0.75639985, -0.85541074, -0.63871258,\n",
            "        1.        ]), array([-0.35424856, -0.24891439, -0.30971049, -0.46014481,  1.81042883,\n",
            "        1.17042472, -0.50909549,  0.1060353 , -0.37465481,  1.37966758,\n",
            "        0.13508531, -0.08682257,  0.15343017, -0.13128594, -0.5893502 ,\n",
            "       -0.12229407, -0.59164451,  0.10444079, -0.28844517, -0.1875763 ,\n",
            "       -0.25247291, -0.21287318, -0.23683354, -0.36192457,  0.58001047,\n",
            "        0.26612877, -0.70805922, -0.07622775, -0.51567681,  0.27620256,\n",
            "        1.        ]), array([-0.98760022,  1.3800326 , -0.98687738, -0.8756682 ,  0.01492473,\n",
            "       -0.60646599, -0.81619008, -0.84524677,  0.31172255,  0.06980143,\n",
            "       -0.5611313 ,  0.50102485, -0.67772623, -0.52137625,  0.04934236,\n",
            "       -0.84550471, -0.69907327, -0.90044809,  0.12564331, -0.44478722,\n",
            "       -0.83230399,  1.54909664, -0.87216547, -0.74690745,  0.76850481,\n",
            "       -0.72815761, -0.76610926, -0.8107588 ,  0.82222776, -0.13719944,\n",
            "        1.        ]), array([-0.70642616, -0.22331665, -0.69195555, -0.68937948,  1.26957147,\n",
            "       -0.05005056, -0.22723639, -0.3628993 , -0.03876801,  0.3405638 ,\n",
            "       -0.35793278,  0.79857725, -0.35199608, -0.43380945,  0.49969396,\n",
            "       -0.13291308, -0.08102636,  0.35424367, -0.59235221,  0.01705767,\n",
            "       -0.64800054,  0.58343296, -0.64787811, -0.63088521,  1.59700315,\n",
            "        0.07465072,  0.07249786,  0.10953674, -0.15329395,  0.38925083,\n",
            "        1.        ]), array([-0.50193594,  0.58417564, -0.50206873, -0.53693558, -0.61488943,\n",
            "       -0.18744878, -0.35969133, -0.29583495,  0.43220368,  0.17612173,\n",
            "       -0.3875283 ,  0.51735394, -0.32823461, -0.42654888,  0.02367466,\n",
            "       -0.03063523, -0.16126635,  0.18392352, -0.1165137 ,  0.25270826,\n",
            "       -0.5051136 ,  0.78535926, -0.47065237, -0.53771584, -0.08629507,\n",
            "       -0.05066881, -0.13811343,  0.08974217,  0.11525763,  0.48013277,\n",
            "        1.        ]), array([-0.53885779,  0.06291261, -0.55314457, -0.55144051, -0.03560273,\n",
            "       -0.44480987, -0.5891962 , -0.20246073,  0.61109991, -0.37816145,\n",
            "       -0.18685622,  0.19802942, -0.2762564 , -0.28815813,  0.15768009,\n",
            "       -0.42968654, -0.59297079, -0.06425726, -0.66742089, -0.1728245 ,\n",
            "       -0.37879325,  0.43687355, -0.45010014, -0.4257368 ,  0.46165357,\n",
            "       -0.31848363, -0.64521166, -0.1005903 , -0.37654768, -0.12223716,\n",
            "        1.        ])]\n",
            "negative_support_vectors: [array([ 0.15697699,  0.19555543,  0.11413667,  0.08421643,  0.16437216,\n",
            "       -0.6129095 , -0.18643273,  0.09468595, -0.82372084, -0.50716342,\n",
            "        0.24372253,  0.04199584,  0.16283575,  0.11139292, -0.44101085,\n",
            "       -0.774525  , -0.39502337, -0.11454226, -0.78002389, -0.64677343,\n",
            "        0.57999886,  0.8472399 ,  0.48070731,  0.45251639,  0.61507919,\n",
            "       -0.42726352,  0.0921677 ,  0.7048967 ,  0.20747112, -0.09896252,\n",
            "        1.        ]), array([-0.24064288,  0.23046144, -0.19149462, -0.31196707,  0.55080051,\n",
            "        0.74401643,  0.12141516,  0.32657385,  0.59284519,  0.71197607,\n",
            "       -0.12549965, -0.71113831, -0.21734776, -0.19817114, -0.65635291,\n",
            "       -0.22401303, -0.33301973, -0.36272305, -0.40589132, -0.18076778,\n",
            "        0.23002939,  0.37824978,  0.17391318,  0.04679014,  0.90439607,\n",
            "        0.75150337,  0.45198197,  0.52674551,  1.35609536,  1.03927837,\n",
            "        1.        ]), array([ 1.10842455, -0.56772258,  1.05162563,  0.95308978, -0.49034991,\n",
            "        0.35740625,  0.25361901,  0.35133607, -0.33449443, -0.7084632 ,\n",
            "       -0.38464093, -1.13623908, -0.46783325, -0.10048355, -0.97469916,\n",
            "       -0.55208463, -0.07107925, -0.33141658, -0.91926417, -0.51400721,\n",
            "        0.85127698, -0.59555609,  0.77558711,  0.72323495, -0.26602223,\n",
            "        0.07846755,  0.75230694,  0.59221988, -0.09505385, -0.0939751 ,\n",
            "        1.        ]), array([ 0.54039616, -0.87954958,  0.56970028,  0.39337038, -0.1032099 ,\n",
            "        0.62083181,  0.39674568,  0.55433471, -0.10813594, -0.4306128 ,\n",
            "       -0.24568635, -0.85138709, -0.11735157, -0.15548782, -0.40067588,\n",
            "        0.38965411,  0.17428272,  0.45319157, -0.71100915, -0.25490504,\n",
            "        0.51580328, -0.60206984,  0.50751457,  0.33297833,  0.4879551 ,\n",
            "        1.23115272,  1.07182201,  1.27132612,  0.19129332,  0.4042131 ,\n",
            "        1.        ]), array([-0.47637467, -0.8353353 , -0.38714807, -0.50565045,  2.23742148,\n",
            "        1.24433549,  0.8663016 ,  0.82465565,  1.0054018 ,  1.89000504,\n",
            "       -0.25507029, -0.59266165, -0.32130419, -0.28925822,  0.1563467 ,\n",
            "        0.44554365,  0.1600252 , -0.06912355,  0.13411881,  0.48684584,\n",
            "       -0.16549825, -0.31383633, -0.11500946, -0.24432021,  2.04851283,\n",
            "        1.72161644,  1.2632432 ,  0.90588779,  1.75406939,  2.24180161,\n",
            "        1.        ]), array([-0.04183295,  0.07687501, -0.03497186, -0.15753229,  0.68601485,\n",
            "        0.1697866 ,  0.2988169 ,  0.40524549, -0.52069254,  0.3745863 ,\n",
            "       -0.66543747, -0.47817655, -0.62574801, -0.47275248, -0.57568298,\n",
            "       -0.42297979, -0.33301973, -0.36158758, -1.0064407 , -0.35892416,\n",
            "        0.15962132,  0.8342124 ,  0.19774185, -0.01983475,  1.26823397,\n",
            "        0.65226558,  0.64628167,  1.03683652,  0.45013821,  1.19444266,\n",
            "        1.        ]), array([-0.04183295,  0.07687501, -0.03497186, -0.15753229,  0.68601485,\n",
            "        0.1697866 ,  0.2988169 ,  0.40524549, -0.52069254,  0.3745863 ,\n",
            "       -0.66543747, -0.47817655, -0.62574801, -0.47275248, -0.57568298,\n",
            "       -0.42297979, -0.33301973, -0.36158758, -1.0064407 , -0.35892416,\n",
            "        0.15962132,  0.8342124 ,  0.19774185, -0.01983475,  1.26823397,\n",
            "        0.65226558,  0.64628167,  1.03683652,  0.45013821,  1.19444266,\n",
            "        1.        ]), array([ 0.60287928,  0.05127727,  0.73446109,  0.4576471 ,  0.44405235,\n",
            "        1.61009904,  1.69241869,  1.10967915,  1.23906218,  0.42278484,\n",
            "       -0.32653265, -0.41939181,  0.10095693, -0.15878808,  0.71870284,\n",
            "        1.63319636,  1.6212552 ,  1.67949664,  0.87027611,  0.9683598 ,\n",
            "        0.23210021, -0.42782699,  0.44198572,  0.1039223 ,  0.23370693,\n",
            "        1.22097449,  1.52326897,  0.95765822,  0.67500972,  0.410863  ,\n",
            "        1.        ])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7UHtmEjN3RE",
        "outputId": "5d957388-ccd1-445b-ed5c-eb65079e2fa9"
      },
      "source": [
        "def find_all_support_vectors(x_train,weights,margin_distance):\n",
        "\n",
        "  all_support_vectors = []\n",
        "  for i in range(len(x_train)):\n",
        "    distance = np.abs(round(np.sum(x_train[i]*weights),1))\n",
        "    if distance <= margin_distance:\n",
        "      all_support_vectors.append(x_train[i])\n",
        "  \n",
        "  all_support_vectors = np.array(all_support_vectors)\n",
        "\n",
        "  return all_support_vectors\n",
        "\n",
        "all_support_vectors = find_all_support_vectors(x_train,weights,max_margin_distance/2)\n",
        "print(len(all_support_vectors))\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrAOHDr2szdl"
      },
      "source": [
        "**Regularization parameter = 0.01**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d2iQ2zRbsi-3",
        "outputId": "6a004e0c-1601-4410-c7a1-5d4cb05c5bd3"
      },
      "source": [
        "num_iterations = 1500\n",
        "weights,accuracies,positive_sv,negative_sv,positive_sv_count,negative_sv_count = soft_svm_sgd(x_train,y_train,num_iterations,0.01)\n",
        "\n",
        "print(\"number_of_positive_suuport_vectors:\",positive_sv_count)\n",
        "print(\"number_of_negative_suuport_vectors:\",negative_sv_count)\n",
        "\n",
        "iter = np.arange(0,num_iterations)\n",
        "plt.plot(iter,accuracies)\n",
        "\n",
        "\n",
        "optimized_bias = weights[-1]\n",
        "optimized_weights = weights[0:-1]\n",
        "print(optimized_weights)\n",
        "print(optimized_bias)\n",
        "\n",
        "### metrics on train set ###\n",
        "\n",
        "print(\"\\nmetrics on train set\\n\")\n",
        "\n",
        "y_pred_train = np.sign(np.matmul(x_train[:,:-1],optimized_weights)+optimized_bias)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "\n",
        "y_pred_train_acc = []\n",
        "y_train_acc = []\n",
        "for i in range(len(y_pred_train)):\n",
        "\n",
        "  if y_pred_train[i] == -1:\n",
        "    y_pred_train_acc.append(-1)\n",
        "  if y_pred_train[i] == 1:\n",
        "    y_pred_train_acc.append(1)\n",
        "\n",
        "  if y_train[i] == -1:\n",
        "    y_train_acc.append(-1)\n",
        "  \n",
        "  if y_train[i] == 1:\n",
        "    y_train_acc.append(1)\n",
        "\n",
        "y_pred_train_acc = np.array(y_pred_train_acc)\n",
        "y_train_acc = np.array(y_train_acc)\n",
        "# print(y_pred_train_acc)\n",
        "# print(y_train_acc)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train_acc, y_pred_train_acc).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy_lp:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "\n",
        "\n",
        "\n",
        "### metrics on test set ###\n",
        "\n",
        "print(\"\\nmetrics on test set\\n\")\n",
        "\n",
        "y_pred_test = np.sign(np.matmul(x_test[:,:-1],optimized_weights)+optimized_bias)\n",
        "# y_test_true = np.array(y_test)\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "y_pred_test_acc = []\n",
        "y_test_acc = []\n",
        "for i in range(len(y_pred_test)):\n",
        "\n",
        "  if y_pred_test[i] == -1:\n",
        "    y_pred_test_acc.append(-1)\n",
        "  if y_pred_test[i] == 1:\n",
        "    y_pred_test_acc.append(1)\n",
        "\n",
        "  if y_test[i] == -1:\n",
        "    y_test_acc.append(-1)\n",
        "  \n",
        "  if y_test[i] == 1:\n",
        "    y_test_acc.append(1)\n",
        "\n",
        "y_pred_test_acc = np.array(y_pred_test_acc)\n",
        "y_test_acc = np.array(y_test_acc)\n",
        "print(y_pred_test_acc)\n",
        "print(y_test_acc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test_acc, y_pred = y_pred_test_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test_acc, y_pred_test_acc).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy_lp:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test_acc, y_pred = y_pred_test_acc))\n",
        "\n",
        "def finding_margin(y_pred_train,optimized_weights,optimized_bias):\n",
        "\n",
        "  max_margin_distance = 2 / np.linalg.norm(optimized_weights)\n",
        "\n",
        "  return max_margin_distance\n",
        "\n",
        "max_margin_distance = finding_margin(y_pred_train,optimized_weights,optimized_bias)\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "\n",
        "print(\"positive_support_vectors:\",positive_sv)\n",
        "print(\"negative_support_vectors:\",negative_sv)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_of_positive_suuport_vectors: 9\n",
            "number_of_negative_suuport_vectors: 2\n",
            "[-0.36491565 -0.4189945  -0.31918067 -0.42136985 -0.07439961  0.34350706\n",
            " -0.38209121 -0.60305621  0.22594185  0.32552292 -0.7778166   0.25448948\n",
            " -0.28488534 -0.65968822 -0.31516552  0.32646633  0.24749861  0.0939862\n",
            "  0.00963952  0.72787099 -0.52647474 -0.89198598 -0.31580391 -0.55436349\n",
            " -0.53161704 -0.08855076 -0.52441407 -0.58436365 -0.7993491  -0.19995838]\n",
            "0.4\n",
            "\n",
            "metrics on train set\n",
            "\n",
            "447\n",
            "8\n",
            "confusion_matrix:\n",
            " [[164   5]\n",
            " [  3 283]]\n",
            "164 5 3 283\n",
            "training accuracy_lp: 98.24175824175823 %\n",
            "0.9824175824175824\n",
            "\n",
            "metrics on test set\n",
            "\n",
            "112\n",
            "2\n",
            "[ 1 -1 -1  1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1  1  1 -1  1  1 -1  1\n",
            "  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1\n",
            "  1  1 -1 -1  1  1  1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1  1  1 -1 -1\n",
            "  1 -1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1 -1  1  1  1  1\n",
            " -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1]\n",
            "[ 1 -1 -1  1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1  1  1 -1 -1  1 -1  1\n",
            "  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1\n",
            "  1  1 -1 -1  1  1  1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1  1  1 -1 -1\n",
            "  1 -1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1\n",
            " -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1]\n",
            "confusion_matrix:\n",
            " [[42  1]\n",
            " [ 1 70]]\n",
            "42 1 1 70\n",
            "testing accuracy_lp: 98.24561403508771 %\n",
            "0.9824561403508771\n",
            "max_margin_distance 0.7883477593965311\n",
            "postive_margin_distance, negative_margin_distance: 0.39417387969826556\n",
            "positive_support_vectors: [array([-0.02195195,  1.82915656, -0.0242624 , -0.1549726 ,  0.20849473,\n",
            "        0.15652057, -0.55467003, -0.15164658, -1.00261707, -0.15418001,\n",
            "       -0.14679399,  0.49921051,  0.01086135, -0.23095369,  0.0716766 ,\n",
            "        0.07276042, -0.37081874,  0.72408171,  0.03120208,  0.57081764,\n",
            "       -0.20070228,  1.22015218, -0.21032414, -0.30567136, -0.36246119,\n",
            "       -0.17726061, -0.66967903, -0.14931541, -1.05277997, -0.04077591,\n",
            "        1.        ]), array([-0.02195195,  1.82915656, -0.0242624 , -0.1549726 ,  0.20849473,\n",
            "        0.15652057, -0.55467003, -0.15164658, -1.00261707, -0.15418001,\n",
            "       -0.14679399,  0.49921051,  0.01086135, -0.23095369,  0.0716766 ,\n",
            "        0.07276042, -0.37081874,  0.72408171,  0.03120208,  0.57081764,\n",
            "       -0.20070228,  1.22015218, -0.21032414, -0.30567136, -0.36246119,\n",
            "       -0.17726061, -0.66967903, -0.14931541, -1.05277997, -0.04077591,\n",
            "        1.        ]), array([-0.74334801,  1.07984094, -0.71872918, -0.71497641, -0.26689042,\n",
            "       -0.04246997,  0.28123994, -0.20297661, -1.54660764,  0.41144401,\n",
            "       -0.60047169,  3.06106409, -0.46040779, -0.51411569,  0.38635603,\n",
            "        0.24266462,  0.84504929,  0.14174901, -0.68558266,  0.35672738,\n",
            "       -0.78467501,  1.86989891, -0.74408636, -0.71438607, -0.11259661,\n",
            "       -0.01631726,  0.43567039, -0.27523937, -1.2760337 ,  0.1869831 ,\n",
            "        1.        ]), array([ 0.14561642, -0.94238039,  0.15656258, -0.00850132,  1.19840603,\n",
            "        0.56018708,  0.13635558,  0.56026733,  1.11127916,  0.0939007 ,\n",
            "        0.38375988, -0.87025627,  0.4692597 ,  0.0533084 , -0.51101369,\n",
            "        1.04188503,  0.41334485,  0.71921542,  0.45134449,  0.39568727,\n",
            "        0.01466355, -1.21110562,  0.06370557, -0.13532962, -0.20465198,\n",
            "        0.34755465, -0.05655553,  0.38209284,  0.40484036,  0.04345613,\n",
            "        1.        ]), array([ 1.05730199, -1.41012088,  0.93217405,  0.95906239, -1.27957466,\n",
            "       -0.79920254, -0.55680438, -0.184147  , -2.15996613, -1.46971658,\n",
            "        0.28234108, -0.30998687,  0.14699477,  0.2335024 , -0.89069575,\n",
            "       -0.9615314 , -0.67520021, -0.70709417, -0.91078868, -0.94029647,\n",
            "        0.73531076, -1.18179374,  0.59091491,  0.57908611, -1.4793997 ,\n",
            "       -0.98286795, -0.80305019, -0.47501233, -1.80828352, -1.39846344,\n",
            "        1.        ]), array([ 0.0831333 , -0.63986166,  0.08983445, -0.03893322,  0.08182025,\n",
            "        0.18115749, -0.10959631, -0.17253971,  0.3664867 ,  0.18746257,\n",
            "       -0.20237583, -0.80965718, -0.0935901 , -0.18034975, -0.8213596 ,\n",
            "        0.28234619,  0.215729  ,  0.10281869, -0.21943042, -0.1595857 ,\n",
            "        0.08300079, -0.67860643,  0.12327725, -0.03249173, -0.13013097,\n",
            "        0.52694605,  0.49563943,  0.40797805,  0.24629786,  0.20471616,\n",
            "        1.        ]), array([ 0.14561642, -0.94238039,  0.15656258, -0.00850132,  1.19840603,\n",
            "        0.56018708,  0.13635558,  0.56026733,  1.11127916,  0.0939007 ,\n",
            "        0.38375988, -0.87025627,  0.4692597 ,  0.0533084 , -0.51101369,\n",
            "        1.04188503,  0.41334485,  0.71921542,  0.45134449,  0.39568727,\n",
            "        0.01466355, -1.21110562,  0.06370557, -0.13532962, -0.20465198,\n",
            "        0.34755465, -0.05655553,  0.38209284,  0.40484036,  0.04345613,\n",
            "        1.        ]), array([ 0.02633046,  0.8913485 ,  0.0988963 , -0.12795363,  0.79276301,\n",
            "        2.59557597,  1.37226693,  0.44522617,  2.08973199,  1.78935515,\n",
            "       -0.43011698,  1.62047503, -0.3732824 , -0.18959047,  0.30968625,\n",
            "        6.14348219,  2.80860819,  0.78896558,  3.70351309,  2.99767825,\n",
            "       -0.10958596,  1.87315579, -0.02565194, -0.20775562,  0.91754684,\n",
            "        4.31579421,  2.76630732,  0.95309024,  3.66466829,  3.36120112,\n",
            "        1.        ]), array([-0.17531962, -0.09300089, -0.15936626, -0.27527814,  0.6788983 ,\n",
            "        0.19631867, -0.03765632,  0.12615461, -0.0205133 , -0.28459959,\n",
            "       -0.69142378,  0.20891548, -0.66980574, -0.46307172, -0.53301458,\n",
            "       -0.33020316,  0.03800735,  0.30395868, -0.89504847, -0.50341617,\n",
            "       -0.30424354,  0.24797475, -0.2958095 , -0.36104561,  0.45726998,\n",
            "        0.01739814,  0.34355794,  0.46736178, -0.37978324, -0.39266635,\n",
            "        1.        ])]\n",
            "negative_support_vectors: [array([-0.78595014, -0.40017375, -0.80234529, -0.72549959, -0.55368715,\n",
            "       -0.97052391, -0.765468  , -0.72014596, -0.76895669, -0.51992186,\n",
            "       -0.77082641,  0.61351417, -0.74604045, -0.58694135, -0.55001527,\n",
            "       -0.8680282 , -0.67188451, -0.96095229, -0.86356806, -0.8079085 ,\n",
            "       -0.66663797,  1.73311013, -0.66098387, -0.63158838,  0.56685971,\n",
            "       -0.58566231, -0.43699914, -0.42065338,  0.11687541, -0.35997102,\n",
            "        1.        ]), array([ 0.4892736 ,  1.08449508,  0.48320086,  0.3635073 , -0.87891322,\n",
            "       -0.07847778,  0.13284018,  0.12176963,  0.12917538, -1.33504419,\n",
            "       -0.00675664, -0.25192787,  0.01828681, -0.08266216,  0.90937723,\n",
            "        0.32314556,  0.61726054,  1.31776909,  1.1221194 , -0.29991695,\n",
            "        0.11820482,  0.32288289,  0.14114875, -0.00717778, -0.84465599,\n",
            "       -0.39354812, -0.19184569, -0.04120657, -0.14844061, -1.16793364,\n",
            "        1.        ])]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfnklEQVR4nO3deXxcdb3/8ddnJnu6JU1oQ1qatlSgFroQyioiZSmIFBUU9HqL4uX+3H6i/K6AG27395Ar4soDRFGLP64gyJVaQawVEKGUttCdLum+N13Tps06398fc2Y6k07SJDOTzDm+n49Hm5kzZ2Y+Ocm8853P+Z4z5pxDRESCJdTfBYiISOYp3EVEAkjhLiISQAp3EZEAUriLiARQXn8XAFBRUeFqamr6uwwREV9ZvHjxXudcZarbciLca2pqWLRoUX+XISLiK2a2ubPb1JYREQkghbuISACdNNzN7JdmtsfMViQsKzezuWa2zvta5i03M/uxmdWZ2TIzm5LN4kVEJLXujNx/DUzvsOxuYJ5zbhwwz7sOcA0wzvt3O/BQZsoUEZGeOGm4O+f+DuzvsHgGMMu7PAu4IWH5Yy7qdWCImVVlqlgREeme3vbchznndnqXdwHDvMvVwNaE9bZ5y0REpA+lvUPVRU8r2eNTS5rZ7Wa2yMwW1dfXp1uGiIgk6O08991mVuWc2+m1XfZ4y7cDIxPWG+EtO4Fz7hHgEYDa2lqdd1gkS15eW8/iTR07q53LD4f48HkjKcwLZ6yGwvwQRfmZezw5ud6G+2xgJvBd7+uzCcs/a2ZPAOcDhxLaNzlj24GjnDq4mFDI+rsUX3HOsXFvI8da2xkxpITBJfk9uv/Ti7cxf/2+bq+fFzI+ddlYaipKT7ittT3Cfc+v5pRBhdx+6dge1eGcY319I81t7eSHQ4w7ZQBm6f0ubNzbSMQ5xlSUpv1YmfaN2SvZuLeR7pQV+3iH789dm9EaivJDvPKly6kcWJjRx+3Myh2H+PWrmxg1tITPXj4u7cfbuLeRoy1tABTmhRhbOYD6I838YO46Wtoi8fXyQsan3zOWUUOP/84659jV0ERjcztjK4//fqze1cCjr2zk/VOquWhsRdo1dnTScDez3wKXARVmtg24l2io/87MbgM2Ax/yVn8OuBaoA44CH894xWlavHk/H3xoPl+7bjy3XTK6v8vJuoamVt7YsJ/qsmLOqhqU1mMt3nyAGx+eD8DZ1YP54+cu6dH9f/q3dexuaKa8tOCk6zrn2HGoiScXbeWCMeUn3L6+vpH6w80AnFZeyvQJw7tdx8tr67n1Vwvj10MGU0ef+BwAk0aWcfc1Z3b5eN+es4pH/7ERgP+4+gw+857TAZi/fh8/+ds6Is6RFwpxw+RqBhfnkxcyLhw7tMuR7Lrdh9m07+gJy9sjjicWbqGptT3l/QYW5fPAhyYysOj4H959R5q59aIavnH9O7v8PmL+tGwnuxuaurVud2zZf5Rfv7aJuj1HMhLu+xtb+I+nljKoOJ/v3XgOeeETu8sPv7yBPy7dAcBzy3cxqLj3B+PvO9LCuj1HkpaNKCtm24FjAAwtLaAoP9zp7+ybmw/S0h79A1A9pJiR5cUAvL4h+m7q4tMzH+zQjXB3zt3SyU3TUqzrgM+kW1Q2ra9vBGDVjgYamlqZvWQHbe0RzqwaxAVjhvZzdZn3k3nr+Pkr0eD54YcnMbg4n2/NWUV7xDG+alDK4OzM0m2HAKgdVcaa3Yd7XEtLW4T3nlPF/TdN7Nb6D8xdy+sb9hFJ0bQbXVHK0NICVu86zEMvr2fXoWPdrmPhpgMAfP+midz51FIiDvYcbqZiQHLwbD9wjDe3HOSu6Wd0ORqf7YUIwPdeWMMtU0+jvLSAv769m9c37OOdpw5m+fb9/KNub3y9r103nvedU8XMXy2ksbkt6fEcjq37u/5+TisvYfjgoqRlW/YdZVdDE2d/4y+cVl4SX97Q1MaQHrzLeu85mZ3gtu1ANNxv+fnrnFZewtnVgzmvpgyA4oIwH5gygvwUAd2ZNzbuY97qaCf4+RU7OWVgdDtMHDmEH988idlLd/DKunouGFNOQV6Yptb2lL9D3VVWWsAlp1fw4fNGkh8O8eTCLTS2tHPqkGLOHD6Qb82YEF/3+39Zw4KN+5Oer7QwTMtRL9zLiuO3TR1dzrQzT+GGydmZc5IT55bpS7G/5gOL8pi9ZAdf/UP02KyKAYXM/cKlXHb/S3xp+hl89PxRaT/XA3PX8uCLdQCEzbj/QxO5fuKpaT/uxr2NPLFwS/wt9KihJSnrdc6xZOtBCsIhWtoj3PHkkmgtIaOspIA/r9zFn1fu6tFzVwwo5JJxFSzafIDnlu/k2rO7HwQt7Y6CvO6/iL945TtOus4XnlzC/7y1naVbD3b7cSE68vrAlGrufGopAHdPP5Or3pk8+v/53zfwn8+9zbfmrIqHz0tr9sQHCDHtEccdV4yjpCDM/31uNQePtlBeWkBre4RBxfn88XOXsHX/UQ4dawXg1l+9wbfnrOLbc1YBMLWmnOqy4vjjveD9TG69qIYbzx1xQu2FeSHGDRt4wvK29gjffX41+xpbkpZPHV3O+zLwe9dbI8pKuGv6mazdfZi/r63nT8t38qflx7u1d/1+OeEULdJwyPjglOqkdyEAS7ZEf9bvm3gqed791u05zJxlOzjc1MpLa6ITND5x8egTfqaZ0NW7xDuvOiPl8k17G6kYWMiAwr6L3H+6cF+xPTr6HFScH39re+O5I5i9ZAcb9h7h0LFWHnttc6/D3TnHyh0NNLdFeHH1HqoGF3HDpGpmvbaJH8xdS3F+mCvHR2eOzlm2g+eX7wKDmRfWdNoa6OiJhVv42csbKM4P0xaJ0NruGFVeSnFB8tv8B1+sY+GmA1x8+lA+P+0d/GDuWuZv2EdRXog3vjwtHjY9UVwQ5uDRVn7413Us23aoZ+He1k5BD0Zo3fH9myby9evG9/h+JYXhpNF4RYp2waTThjCoKI8n3jg+u9cMzh1VxtSa4z+rcMi4+bzTWO79bh1tif5etba7+B+FkeUl8ZkG933wHN7yAmrYoEL+5YJRSbU0NLXy3wu28OHakZR1o4UVkxcO8dVebIu+8KnLovtFIhEX/71zwDNvbuPg0RN/Dw83tfLMm9v5w1s7kpYf816zA4vy+Mktk+PLF27azydnLeL1DfsYWlrA7z91Ucp9Nf2lP2r5pwp35xyNze2xK7R5749OGVhIS3uEv6zcDdBpT3jP4aZ4nyyVxZv2M2fZzqSR003njuD/XH0GrZEIP3t5A48v2BwP95+/spG63YdpbotQkh/udrg3tbQzuDifpfdexSvr6vnYo2/wL48uSLlucX6Yb14/gdNPGcD0CcOZ77U5QiHrUXAkGj44TNXgIp5evJXXN3S+g3RQcT4PfmRyfOTV0h7p0ci9O9L5PgCe+fRFbD9wjEkjhpxw23k15Sz7xtXdfqwN9dG+bKzN0toeSfnHbNpZw5h21rATlscMKsrnf727ZzuJ/aLjz+uT7xrT6brfTGh3xLxWt5dv/nHVCX3q82rKWXrvVZkrNAD+KcK9qbWdDz70GrsbmuM7Ntoijjbv8uDiaPi8uCbaxzv9lAEpH+e//ryGpxdvO+nzTX/ncG6eOhIzi4fGPdecxeJNB2hpi7Bk60Fe37CPzfsauXrCcFZsP0RDU/dH0c1tEQq9kLx4bAW/+/cL4yOajsZXDYrvxBpYFP1xt7v0Z57edslo/r5ub6e3729s5u9r69m4t5FzvG0QHcnm1kySKaeVMeW0sow8Von3lvvV9fs4f8xQWtsjOff9+t1Fp1fwwhcu7e8yfCGw4V635wgrdxxixqRq9jW2sHJHA+ePLufas4fz2PzNtEeOj9zPrBpEyGDt7ujIq7OdO/WHmzlz+EB++pETz4d2pLmNGx58FYCHP3ZuyvsX5odoao3w7TmrWLw5ulNvwqmD2br/KMu2HeJrf1iRtH5JQZjPTRt3Qp+uqbU9PtMiFLJuj/jHVkb/aL37HSnP7d8jn3zXmC5HXa/W7eWjv1hAU2v0D2h7xNEecRSEgzvXebw3G2m9N4KPhrtOvCr9I7DhfsUDLwMwY1I1rd481JunjuT9k0fwzJvbvZG7I2TRsHv729OJRGDqf/4Vl+KA20PHWnm1bi8Xjh2acmQfexfQlYJwiIZjbTS3tXPFWcP46UcmU5QfpqGplcfmb07aydTWHqGhqY3amvJ4GycmceTeExNHDmHNd6ZnvO+dSqy+5rboO4qHXoruWM50WyaXFOSFGFNRyp+W7eQ7M1poaXMKd+k3gQ33RK1e8MZeaOGQ0R5xtEYi8Tmy8aPx7PiBHIm+M2cVbRHH8EFFJ94IKefadlSYF6alLcKx1nYGFuXFR993XPEO7rgieWbIjoPHuOi7f+MvK3dx4Gjy7IdN+472+mi/TB512JVYfbGR+2Pzox8Yc34Ppl760QemVHP/X9ayq6EpOnIP8B8zyW2BD/dIxMX77LFwzwsZbZEIee1GfocpWJ11SA94e/Tv7eJAkC9NPyM+5zaVgrzolMRjLe0nzGzpqGJAIYOL83lq8TaeStHnv2p85zvkckHHkXtDUyv/9q7RGetv56rJ3vd3zY9eIRwyzg349yu5K1DhfqS5ja37k4/qa2mPThUE4u2I2Mi9LeK6NeKGaEhNGjmky3mqn77s9C4foyAvxG5vRFdykpF3QV6If9z1nk6nK3b1RyQXxEbuj722mRdX19PUGonvuA6yQQlzsmdeWMN7zkx//4ZIb/g+3JdvO0R1WTHlpQXc/tgiXutw7pLmtsgJbZm8kHn99kj8IIgYM8Ol6Mu09LLPneiCMUOZv34fZnBeN3aCDizKP+EADr+oHFjIxJFD2H7wGNsPHmPU0BLOHRXslgxEz6ES8/X35eacc/nn4Otw33ukmff99B9cOX4YP//XWuoPNzPltCHcfukYfvP6Zl6t20dLWyS+QzU2LS0cNtbuOUJeyMgLdwx3qD/SzG9e3xwP+ZFlJTS3RY82TMeN545IecRhEBXlh3n2Mxf3dxl9Lsg7jMVffB3u67ypi29sjB5Y1NwWYcLQUqZPqPJmt+xj56Fj1B+JnmAqtnOrekhx/GCkKaclH7xiRE809Nzy44flhyx6Lo++OqOd+JfCXXKFr8M9NrKOdVaaEw5vL/V649f/9NX4+iXeTszf3HY+DV4vu7PReEFeiPl3X87zK3bx1T+sYOuBY0yoHpyV70OCoy+mmYp0h6/DPebA0VYWbtofnf/t9TyvOGsY9980MT5bY1BRPmd4J1vKD4cYOiD1KDx2jo+QwdABhbz7HZVMqB7E0ZZ2Lh2nnWPSNY3cJVf4OtwTd3ve5J1nPLbTsyg/3Kv+tsW/Ri+NLC9hzufelU6Z8k9E4S65InC/iZk6SEcf0iS9obaM5Apfj9w7GlpawNkj0uuLx868mmsflSb+YGaMrSzt8rw7In0hMOF+VtUgnv98JtonlvC/SM/Nu/Oy/i5BxN9tmcRjjTJ+alWlu4j4mK/DPVHHI017K9aNCaktIyI+Fpxwz9COrPhsGWW7iPiYr8M98bzrmW7LKNtFxM98He6J8kIZGrmrLSMiARCYcM/UyD128JKyXUT8LDDhnumRuxozIuJnvg732FTI4vww1086NaOPrSNURcTPAnEQ0//75NSMfRCEZsuISBD4euSeDbHTDpjaMiLiYwr3TmjkLiJ+5utwP/GTTjNHUyFFxM98He7HZS6IlekiEgQBCffMU8iLiJ+lFe5m9gUzW2lmK8zst2ZWZGajzWyBmdWZ2ZNmVpCpYjtyLvONGR2hKiJB0OtwN7Nq4H8Dtc65CUAYuBm4D/iBc+504ABwWyYK7bqWDD6WjlAVkQBIty2TBxSbWR5QAuwELgee9m6fBdyQ5nP0C2W7iPhZr8PdObcduB/YQjTUDwGLgYPOuTZvtW1Adar7m9ntZrbIzBbV19f3toyMU1tGRIIgnbZMGTADGA2cCpQC07t7f+fcI865WudcbWVlZa9qyMZUSJ1aRkSCIJ22zBXARudcvXOuFXgGuBgY4rVpAEYA29Os8aQymcPHj1AVEfGvdMJ9C3CBmZVYNBGnAauAF4EbvXVmAs+mV2L/MLVlRMTH0um5LyC64/RNYLn3WI8AdwFfNLM6YCjwaAbq7KSIzD9kLNJ1VkgR8bO0zgrpnLsXuLfD4g3A1HQet6cyOsq22Belu4j4l45Q7YS6MiLiZwr3Do6fz13pLiL+5etwd1loumu2jIgEga/DPSYbQayBu4j4WSDCPZP0MXsiEgS+DvcsnBRSpx8QkUDwdbjHZCOHFe0i4meBCPdMis9v18hdRHxM4d7B8bZM/9YhIpIOX4d7Nnrure0RQG0ZEfE3X4d7TCZPFbC+vjH6mGrLiIiPBSLcs+E7N0zo7xJERHrN1+GejQ/riCnOD2fx0UVEssvX4R6TjQ6K5rmLiJ8FItyzQdkuIn6mcO9ESHMhRcTHfB3uLhtzIT3KdhHxM1+Hezap5y4ifqZw74TCXUT8zNfhns2pkGrLiIif+Trcs0kjdxHxs0CEu+a5i4gkC0S4Z4Npy4iIj/k6wrI4E1IjdxHxNV+He0wmzwoZox2qIuJngQj3bNDIXUT8TOHeCWW7iPiZz8M9e033sNJdRHzM5+EepamQIiLJAhHu2aBsFxE/83W4Z3MqpD5DVUT8LK1wN7MhZva0ma02s7fN7EIzKzezuWa2zvtalqliO68j288gIuIv6Y7cfwT82Tl3JjAReBu4G5jnnBsHzPOui4hIH+p1uJvZYOBS4FEA51yLc+4gMAOY5a02C7gh3SJFRKRn0hm5jwbqgV+Z2Vtm9gszKwWGOed2euvsAoalurOZ3W5mi8xsUX19fa8KyOYpf0VE/CydcM8DpgAPOecmA410aMG46Ofgpcxg59wjzrla51xtZWVlGmVk9vQDX79uPLdMHZmxxxMR6Q95adx3G7DNObfAu/400XDfbWZVzrmdZlYF7Em3yL70iUtG93cJIiJp6/XI3Tm3C9hqZmd4i6YBq4DZwExv2Uzg2bQq7LKGbD2yiIi/pTNyB/gc8LiZFQAbgI8T/YPxOzO7DdgMfCjN5zgpTYUUEUmWVrg755YAtSlumpbO44qISHp8fYSqiIik5utwd5oMKSKSkq/DPUYtdxGRZIEIdxERSebrcNdUSBGR1Hwd7jGaCikikiwQ4S4iIskU7iIiAeTrcFfLXUQkNV+H+3FquouIJApIuIuISCJfh7vTXEgRkZR8He4xmgopIpIsEOEuIiLJFO4iIgGkcBcRCaBAhLta7iIiyQIR7iIikszX4a6ZkCIiqfk63GNMcyFFRJIEItxFRCSZwl1EJIB8He76gGwRkdR8He4x6riLiCQLRLiLiEgyhbuISAD5Otw1z11EJDVfh3uMprmLiCQLRLiLiEgyX4e72jIiIqn5OtxjTJMhRUSSpB3uZhY2s7fMbI53fbSZLTCzOjN70swK0i9TRER6IhMj988Dbydcvw/4gXPudOAAcFsGnkNERHogrXA3sxHAe4FfeNcNuBx42ltlFnBDOs/RFbXcRURSS3fk/kPgS0DEuz4UOOica/OubwOqU93RzG43s0Vmtqi+vj6tIjQVUkQkWa/D3cyuA/Y45xb35v7OuUecc7XOudrKysreliEiIinkpXHfi4HrzexaoAgYBPwIGGJmed7ofQSwPf0yU3OaCykiklKvR+7OuXuccyOcczXAzcDfnHMfBV4EbvRWmwk8m3aVIiLSI9mY534X8EUzqyPag380C88hIiJdSKctE+ecewl4ybu8AZiaiccVEZHe8fURquq4i4ik5utwj9FUSBGRZIEIdxERSebvcFdfRkQkJX+Hu8fUlxERSRKIcBcRkWQKdxGRAPJ1uDs13UVEUvJ1uMeo4y4ikiwQ4S4iIsl8He46KaSISGq+DvcYzYQUEUkWiHAXEZFkCncRkQDydbir5S4ikpqvwz3GNBlSRCRJIMJdRESS+TrcNRVSRCQ1X4d7jKZCiogkC0S4i4hIMoW7iEgA+TrcdVZIEZHUfB3uMWq5i4gkC0S4i4hIMoW7iEgA+TrcNc9dRCQ1X4d7nJruIiJJghHuIiKSxNfhrq6MiEhqvg73GJ0VUkQkWSDCXUREkvU63M1spJm9aGarzGylmX3eW15uZnPNbJ33tSxz5YqISHekM3JvA+50zo0HLgA+Y2bjgbuBec65ccA873p2aC6kiEhKvQ5359xO59yb3uXDwNtANTADmOWtNgu4Id0iT0an/BURSZaRnruZ1QCTgQXAMOfcTu+mXcCwTu5zu5ktMrNF9fX1mShDREQ8aYe7mQ0Afg/c4ZxrSLzNOefoZMaic+4R51ytc662srKyV8+tpoyISGpphbuZ5RMN9sedc894i3ebWZV3exWwJ70Su1FHtp9ARMRn0pktY8CjwNvOuQcSbpoNzPQuzwSe7X15IiLSG3lp3Pdi4GPAcjNb4i37MvBd4HdmdhuwGfhQeiWKiEhP9TrcnXP/oPOOyLTePm7PauiLZxER8Z9AHKFqmgspIpIkEOEuIiLJfB3uTn0ZEZGUfB3uMWrKiIgkC0S4i4hIMoW7iEgA+Trc1XEXEUnN1+Eeo5mQIiLJAhHuIiKSzNfhrpmQIiKp+TrcY/QB2SIiyQIR7iIikkzhLiISQL4Od7XcRURS83W4x6nlLiKSJBjhLiIiSXwd7jorpIhIar4O9xgdoSoikiwQ4S4iIskU7iIiAaRwFxEJoECEu1ruIiLJAhHuIiKSTOEuIhJAvg53TXMXEUnN1+EeY5roLiKSJBDhLiIiyXwd7k7nhRQRScnX4R6jpoyISLJAhLuIiCRTuIuIBFBWwt3MppvZGjOrM7O7s/EcoKmQIiKdyXi4m1kYeBC4BhgP3GJm4zP9PMnPmc1HFxHxn2yM3KcCdc65Dc65FuAJYEYWnkdERDqRjXCvBrYmXN/mLUtiZreb2SIzW1RfX9+rJxpTOYD3nl1FSEN3EZEkef31xM65R4BHAGpra3vVPb9y/DCuHD8so3WJiARBNkbu24GRCddHeMtERKSPZCPcFwLjzGy0mRUANwOzs/A8IiLSiYy3ZZxzbWb2WeAFIAz80jm3MtPPIyIinctKz9059xzwXDYeW0RETk5HqIqIBJDCXUQkgBTuIiIBpHAXEQkgczlw9i0zqwc29/LuFcDeDJaTDaoxfbleH+R+jbleH6jGnhrlnKtMdUNOhHs6zGyRc662v+voimpMX67XB7lfY67XB6oxk9SWEREJIIW7iEgABSHcH+nvArpBNaYv1+uD3K8x1+sD1Zgxvu+5i4jIiYIwchcRkQ4U7iIiAeTrcO+rD+I+SQ0jzexFM1tlZivN7PPe8nIzm2tm67yvZd5yM7MfezUvM7MpfVhr2MzeMrM53vXRZrbAq+VJ7xTNmFmhd73Ou72mD2obYmZPm9lqM3vbzC7MtW1oZl/wfsYrzOy3ZlbU39vQzH5pZnvMbEXCsh5vNzOb6a2/zsxmZrm+73k/52Vm9j9mNiThtnu8+taY2dUJy7P2Wk9VY8Jtd5qZM7MK73qfb8Nec8758h/R0wmvB8YABcBSYHw/1FEFTPEuDwTWEv1g8P8C7vaW3w3c512+FngeMOACYEEf1vpF4L+BOd713wE3e5cfBj7lXf408LB3+WbgyT6obRbwSe9yATAkl7Yh0Y+K3AgUJ2y7W/t7GwKXAlOAFQnLerTdgHJgg/e1zLtclsX6rgLyvMv3JdQ33nsdFwKjvdd3ONuv9VQ1estHEj11+Wagor+2Ya+/r/588jR/IBcCLyRcvwe4Jwfqeha4ElgDVHnLqoA13uWfAbckrB9fL8t1jQDmAZcDc7xfzr0JL7L49vR+oS/0Lud561kWaxvsBad1WJ4z25Djnw1c7m2TOcDVubANgZoO4dmj7QbcAvwsYXnSepmur8Nt7wce9y4nvYZj27AvXuupagSeBiYCmzge7v2yDXvzz89tmW59EHdf8t56TwYWAMOcczu9m3YBsQ977a+6fwh8CYh414cCB51zbSnqiNfo3X7IWz9bRgP1wK+8ttEvzKyUHNqGzrntwP3AFmAn0W2ymNzZhol6ut3687X0CaIjYbqoo8/rM7MZwHbn3NION+VMjSfj53DPKWY2APg9cIdzriHxNhf9U95vc07N7Dpgj3NucX/VcBJ5RN8WP+Scmww0Em0nxOXANiwDZhD9Q3QqUApM7696uqu/t1tXzOwrQBvweH/XksjMSoAvA1/v71rS4edwz5kP4jazfKLB/rhz7hlv8W4zq/JurwL2eMv7o+6LgevNbBPwBNHWzI+AIWYW+zSuxDriNXq3Dwb2ZbG+bcA259wC7/rTRMM+l7bhFcBG51y9c64VeIbods2VbZiop9utz7enmd0KXAd81PsDlEv1jSX6R3yp95oZAbxpZsNzqMaT8nO458QHcZuZAY8CbzvnHki4aTYQ22M+k2gvPrb8X7297hcAhxLeQmeFc+4e59wI51wN0e30N+fcR4EXgRs7qTFW+43e+lkb/TnndgFbzewMb9E0YBU5tA2JtmMuMLMS72ceqzEntmEHPd1uLwBXmVmZ9w7lKm9ZVpjZdKItwuudc0c71H2zN9NoNDAOeIM+fq0755Y7505xztV4r5ltRCdN7CJHtmG39GfDP91/RPdcryW6J/0r/VTDJUTf9i4Dlnj/riXaX50HrAP+CpR76xvwoFfzcqC2j+u9jOOzZcYQffHUAU8Bhd7yIu96nXf7mD6oaxKwyNuOfyA64yCntiHwTWA1sAL4DdFZHf26DYHfEt0H0Eo0hG7rzXYj2vuu8/59PMv11RHtT8deLw8nrP8Vr741wDUJy7P2Wk9VY4fbN3F8h2qfb8Pe/tPpB0REAsjPbRkREemEwl1EJIAU7iIiAaRwFxEJIIW7iEgAKdxFRAJI4S4iEkD/Hx/Hy3WRcncWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwhqrjDdPsBN",
        "outputId": "df5f81db-4927-401a-c71c-e88247cda97f"
      },
      "source": [
        "def find_all_support_vectors(x_train,weights,margin_distance):\n",
        "\n",
        "  all_support_vectors = []\n",
        "  for i in range(len(x_train)):\n",
        "    distance = np.abs(round(np.sum(x_train[i]*weights),1))\n",
        "    if distance <= margin_distance:\n",
        "      all_support_vectors.append(x_train[i])\n",
        "  \n",
        "  all_support_vectors = np.array(all_support_vectors)\n",
        "\n",
        "  return all_support_vectors\n",
        "\n",
        "all_support_vectors = find_all_support_vectors(x_train,weights,max_margin_distance/2)\n",
        "print(len(all_support_vectors))\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFR00DH5s51f"
      },
      "source": [
        "**Regularization parameter = 0.001**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QaaWcHRys2O8",
        "outputId": "58ee1bd5-cad3-4ac8-8eed-ab47dbef5532"
      },
      "source": [
        "num_iterations = 1500\n",
        "weights,accuracies,positive_sv,negative_sv,positive_sv_count,negative_sv_count = soft_svm_sgd(x_train,y_train,num_iterations,0.01)\n",
        "\n",
        "print(\"number_of_positive_suuport_vectors:\",positive_sv_count)\n",
        "print(\"number_of_negative_suuport_vectors:\",negative_sv_count)\n",
        "\n",
        "iter = np.arange(0,num_iterations)\n",
        "plt.plot(iter,accuracies)\n",
        "\n",
        "\n",
        "optimized_bias = weights[-1]\n",
        "optimized_weights = weights[0:-1]\n",
        "print(optimized_weights)\n",
        "print(optimized_bias)\n",
        "\n",
        "### metrics on train set ###\n",
        "\n",
        "print(\"\\nmetrics on train set\\n\")\n",
        "\n",
        "y_pred_train = np.sign(np.matmul(x_train[:,:-1],optimized_weights)+optimized_bias)\n",
        "\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_train)):\n",
        "  if y_pred_train[i] == y_train[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "\n",
        "y_pred_train_acc = []\n",
        "y_train_acc = []\n",
        "for i in range(len(y_pred_train)):\n",
        "\n",
        "  if y_pred_train[i] == -1:\n",
        "    y_pred_train_acc.append(-1)\n",
        "  if y_pred_train[i] == 1:\n",
        "    y_pred_train_acc.append(1)\n",
        "\n",
        "  if y_train[i] == -1:\n",
        "    y_train_acc.append(-1)\n",
        "  \n",
        "  if y_train[i] == 1:\n",
        "    y_train_acc.append(1)\n",
        "\n",
        "y_pred_train_acc = np.array(y_pred_train_acc)\n",
        "y_train_acc = np.array(y_train_acc)\n",
        "# print(y_pred_train_acc)\n",
        "# print(y_train_acc)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_train_acc, y_pred_train_acc).ravel()\n",
        "training_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"training accuracy_lp:\",training_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_train_acc, y_pred = y_pred_train_acc))\n",
        "\n",
        "\n",
        "\n",
        "### metrics on test set ###\n",
        "\n",
        "print(\"\\nmetrics on test set\\n\")\n",
        "\n",
        "y_pred_test = np.sign(np.matmul(x_test[:,:-1],optimized_weights)+optimized_bias)\n",
        "# y_test_true = np.array(y_test)\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "for i in range(len(y_pred_test)):\n",
        "  if y_pred_test[i] == y_test[i]:\n",
        "    correct+=1\n",
        "  else:\n",
        "    incorrect+=1\n",
        "    \n",
        "print(correct)\n",
        "print(incorrect)\n",
        "\n",
        "y_pred_test_acc = []\n",
        "y_test_acc = []\n",
        "for i in range(len(y_pred_test)):\n",
        "\n",
        "  if y_pred_test[i] == -1:\n",
        "    y_pred_test_acc.append(-1)\n",
        "  if y_pred_test[i] == 1:\n",
        "    y_pred_test_acc.append(1)\n",
        "\n",
        "  if y_test[i] == -1:\n",
        "    y_test_acc.append(-1)\n",
        "  \n",
        "  if y_test[i] == 1:\n",
        "    y_test_acc.append(1)\n",
        "\n",
        "y_pred_test_acc = np.array(y_pred_test_acc)\n",
        "y_test_acc = np.array(y_test_acc)\n",
        "print(y_pred_test_acc)\n",
        "print(y_test_acc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "print(\"confusion_matrix:\\n\",confusion_matrix(y_true = y_test_acc, y_pred = y_pred_test_acc))\n",
        "TN, FP, FN, TP = confusion_matrix(y_test_acc, y_pred_test_acc).ravel()\n",
        "testing_accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
        "print(TN, FP, FN, TP)\n",
        "print(\"testing accuracy_lp:\",testing_accuracy*100,\"%\")\n",
        "print(accuracy_score(y_true = y_test_acc, y_pred = y_pred_test_acc))\n",
        "\n",
        "def finding_margin(y_pred_train,optimized_weights,optimized_bias):\n",
        "\n",
        "  max_margin_distance = 2 / np.linalg.norm(optimized_weights)\n",
        "\n",
        "  return max_margin_distance\n",
        "\n",
        "max_margin_distance = finding_margin(y_pred_train,optimized_weights,optimized_bias)\n",
        "print(\"max_margin_distance\",max_margin_distance)\n",
        "print(\"postive_margin_distance, negative_margin_distance:\",max_margin_distance/2)\n",
        "\n",
        "print(\"positive_support_vectors:\",positive_sv)\n",
        "print(\"negative_support_vectors:\",negative_sv)\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_of_positive_suuport_vectors: 8\n",
            "number_of_negative_suuport_vectors: 3\n",
            "[-0.30854514 -0.43018624 -0.28895768 -0.37511404 -0.07135788  0.35636113\n",
            " -0.30427276 -0.69474938  0.13965169  0.47730521 -0.65034214 -0.00931446\n",
            " -0.44368334 -0.49335777 -0.38852593  0.45583405  0.39536117 -0.18819862\n",
            "  0.39894077  0.32449407 -0.53991081 -0.73794777 -0.4823964  -0.5760992\n",
            " -0.52280785  0.02783744 -0.29991781 -0.55392579 -0.47870251 -0.0937696 ]\n",
            "0.13333333333333333\n",
            "\n",
            "metrics on train set\n",
            "\n",
            "442\n",
            "13\n",
            "confusion_matrix:\n",
            " [[163   6]\n",
            " [  7 279]]\n",
            "163 6 7 279\n",
            "training accuracy_lp: 97.14285714285714 %\n",
            "0.9714285714285714\n",
            "\n",
            "metrics on test set\n",
            "\n",
            "111\n",
            "3\n",
            "[ 1 -1 -1  1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1  1  1 -1  1  1 -1  1\n",
            "  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1\n",
            "  1  1 -1 -1  1  1  1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1  1  1 -1 -1\n",
            "  1 -1  1  1  1  1  1  1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1\n",
            " -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1 -1 -1]\n",
            "[ 1 -1 -1  1  1 -1 -1 -1  1  1  1 -1  1 -1  1 -1  1  1  1 -1 -1  1 -1  1\n",
            "  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1\n",
            "  1  1 -1 -1  1  1  1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1  1  1 -1 -1\n",
            "  1 -1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1\n",
            " -1 -1  1 -1 -1  1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1]\n",
            "confusion_matrix:\n",
            " [[41  2]\n",
            " [ 1 70]]\n",
            "41 2 1 70\n",
            "testing accuracy_lp: 97.36842105263158 %\n",
            "0.9736842105263158\n",
            "max_margin_distance 0.8569541624199563\n",
            "postive_margin_distance, negative_margin_distance: 0.42847708120997813\n",
            "positive_support_vectors: [array([-0.90239596,  0.47945762, -0.8266475 , -0.80712533,  1.87447773,\n",
            "        0.33087418,  0.19586614,  0.20044127,  0.30442067,  0.83672522,\n",
            "       -0.60841195,  0.3830925 , -0.51684128, -0.51411569,  0.34102086,\n",
            "       -0.43806997, -0.19707594, -0.23133322, -0.8441955 , -0.2855434 ,\n",
            "       -0.7101253 ,  1.57352321, -0.59694432, -0.64442114,  2.56577635,\n",
            "        0.09882403,  0.62421306,  0.42320465,  0.10231538,  0.67131734,\n",
            "        1.        ]), array([-0.27188444,  0.58650271, -0.269756  , -0.35093127,  0.05548903,\n",
            "        0.00680388, -0.07795778,  0.09288037, -0.2395699 , -0.14567439,\n",
            "       -0.76288614, -1.09686782, -0.75841621, -0.56867993, -1.23037619,\n",
            "       -0.65045022, -0.57606071, -0.80182461, -1.13962705, -0.78483517,\n",
            "       -0.00811552,  0.68602455, -0.0524592 , -0.24590233,  0.78603916,\n",
            "        0.86600852,  0.48268612,  0.70185138,  1.28167745,  0.67796724,\n",
            "        1.        ]), array([-0.02195195,  1.82915656, -0.0242624 , -0.1549726 ,  0.20849473,\n",
            "        0.15652057, -0.55467003, -0.15164658, -1.00261707, -0.15418001,\n",
            "       -0.14679399,  0.49921051,  0.01086135, -0.23095369,  0.0716766 ,\n",
            "        0.07276042, -0.37081874,  0.72408171,  0.03120208,  0.57081764,\n",
            "       -0.20070228,  1.22015218, -0.21032414, -0.30567136, -0.36246119,\n",
            "       -0.17726061, -0.66967903, -0.14931541, -1.05277997, -0.04077591,\n",
            "        1.        ]), array([-0.38549012,  2.35972786, -0.43740011, -0.41805209, -0.96787002,\n",
            "       -1.17501038, -0.86415007, -0.87516779, -0.99531518, -0.91118058,\n",
            "       -0.59289235,  0.27241752, -0.68762684, -0.47473263,  0.59803128,\n",
            "       -0.73484342, -0.61717542, -0.40408651, -0.19037158, -0.78256566,\n",
            "       -0.4968303 ,  1.68100011, -0.57073278, -0.50255759, -0.39314631,\n",
            "       -0.94062827, -0.89070094, -0.75563852, -0.79878842, -1.05876448,\n",
            "        1.        ]), array([ 0.03769103,  0.08385621,  0.24141439, -0.07107158, -1.28028631,\n",
            "        2.25444934,  2.65538498,  0.74959516, -0.39290952,  2.11115127,\n",
            "       -0.15292965,  0.49558182,  0.263822  , -0.24393469, -0.58101653,\n",
            "        2.73757366,  3.70053211,  1.80439808,  0.61964361,  3.47427494,\n",
            "       -0.20070228, -0.31709321, -0.00778044, -0.30162816, -1.87962139,\n",
            "        1.0498529 ,  1.94832956,  0.54654009, -0.81334844,  1.34406537,\n",
            "        1.        ]), array([ 0.0831333 , -0.63986166,  0.08983445, -0.03893322,  0.08182025,\n",
            "        0.18115749, -0.10959631, -0.17253971,  0.3664867 ,  0.18746257,\n",
            "       -0.20237583, -0.80965718, -0.0935901 , -0.18034975, -0.8213596 ,\n",
            "        0.28234619,  0.215729  ,  0.10281869, -0.21943042, -0.1595857 ,\n",
            "        0.08300079, -0.67860643,  0.12327725, -0.03249173, -0.13013097,\n",
            "        0.52694605,  0.49563943,  0.40797805,  0.24629786,  0.20471616,\n",
            "        1.        ]), array([ 0.21093969,  0.21417197,  0.17097915,  0.07397766, -0.03489108,\n",
            "       -0.39591506, -0.25812162,  0.01524048, -1.18516424, -0.75382653,\n",
            "       -0.62393156,  0.76047603, -0.62871819, -0.40696736, -0.54201495,\n",
            "       -0.65771586, -0.34362998,  0.00549289, -0.11530291, -0.75003605,\n",
            "       -0.05367368,  0.4564148 , -0.10011654, -0.17048787, -0.47205092,\n",
            "       -0.734519  , -0.49025165, -0.19651786, -0.86026408, -0.99503629,\n",
            "        1.        ]), array([-0.1809999 ,  0.700529  , -0.2083826 , -0.26703025, -0.62912252,\n",
            "       -0.51853113, -0.51838617, -0.38895122, -0.00956047, -0.79635465,\n",
            "       -0.61851774,  0.2470167 , -0.55990894, -0.44305017, -0.86202792,\n",
            "       -0.65156801, -0.36286105,  0.00711499, -0.50033255, -0.69556786,\n",
            "       -0.23176466,  1.00031306, -0.24606715, -0.31955887, -0.70876473,\n",
            "       -0.52904587, -0.21103578,  0.20698697, -0.04813821, -0.81881399,\n",
            "        1.        ])]\n",
            "negative_support_vectors: [array([ 0.0831333 ,  0.11178102,  0.10342722, -0.03523588,  0.0825319 ,\n",
            "        0.18494779,  0.06378784,  0.24429104,  0.24600557,  0.15627528,\n",
            "       -0.41748474,  1.15055996, -0.24259432, -0.2967388 ,  0.32135339,\n",
            "        0.19404072,  0.04364405,  0.39641819, -0.71706307,  0.24854749,\n",
            "        0.01259273,  0.84398302,  0.06668416, -0.09524921,  0.47042075,\n",
            "        0.30747785,  0.22649836,  0.63789967, -0.29565865,  0.53111532,\n",
            "        1.        ]), array([ 0.35294679,  0.80757409,  0.33903516,  0.20850371, -0.31030134,\n",
            "       -0.01404275,  0.29379491,  0.66834411, -0.34544726, -0.25908271,\n",
            "       -0.33302923, -0.68138307, -0.39159853, -0.23975437,  0.94704543,\n",
            "       -0.0580211 ,  0.44484402,  0.94630895,  0.50461903, -0.62521311,\n",
            "        0.03330098,  0.02650719,  0.00711248, -0.0873386 , -0.29232376,\n",
            "       -0.34710992,  0.05954454,  0.50238296, -0.55773911, -0.86813407,\n",
            "        1.        ]), array([ 0.13425586,  0.93090865,  0.08242022,  0.0279032 , -0.67893833,\n",
            "       -0.71979584, -0.06151077,  0.09778122, -0.67403217, -1.22447108,\n",
            "        0.03799756,  0.74414693,  0.02373215, -0.16120827,  0.42369087,\n",
            "       -0.45036567,  0.06619082,  0.64297688, -0.38046483, -0.34946788,\n",
            "       -0.09923184,  0.98240025, -0.15075246, -0.21513886, -0.05122636,\n",
            "       -0.61174404, -0.02249311,  0.32423177, -0.68554377, -0.8637008 ,\n",
            "        1.        ])]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiF58hnDs7f6",
        "outputId": "1f3e68c7-7475-4dc9-b9f9-78d4f4d84d49"
      },
      "source": [
        "def find_all_support_vectors(x_train,weights,margin_distance):\n",
        "  y_s = []\n",
        "  all_support_vectors = []\n",
        "  for i in range(len(x_train)):\n",
        "    distance = np.abs(round(np.sum(x_train[i]*weights),1))\n",
        "    if distance <= margin_distance:\n",
        "      all_support_vectors.append(x_train[i])\n",
        "      y_s.append(y_train[i])\n",
        "  \n",
        "  all_support_vectors = np.array(all_support_vectors)\n",
        "\n",
        "  return all_support_vectors,y_s\n",
        "\n",
        "all_support_vectors,y_s = find_all_support_vectors(x_train,weights,max_margin_distance/2)\n",
        "print(len(all_support_vectors))\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ]
    }
  ]
}